\documentclass[10pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry, esint, float}


\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  

\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}
\newcommand{\scA}{\mathscr{A}}
\newcommand{\curl}{\text{curl}}

\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\input{paolo-pset.tex}



\title{UChicago Markov Chains, Martingales, and Brownian Motion Analysis Notes: 23500}
\author{Notes by Agustín Esteva, Lectures by Stephen Yearwood, Books by }
\date{Academic Year 2024-2025}

\begin{document}

\maketitle
\tableofcontents

\vspace{.25in}


\newpage
\section{Lectures}

\subsection{Monday, Mar 24: Markov Processes: Basic Definitions and Examples}
He is from Trinidad and Tobago. 
\begin{defn}
    A \textbf{stochastic process}  is a collection of random variables $\{X_t\}_{t \in T}$ indexed by time, where each $X_t$ takes values in $S$
\end{defn}
We call $S$ our \textit{state space}. In discrete time, it should be obvious that $T = \bbN_0^*$ (where $\bbN_0^*$ is when on goscale the naturals by a constant) and $T = [0, \bbR)$ in continuous time. We say that $S$ is a discrete space if it is countable and continuous if it is $\bbR^d.$
\begin{rem}
    In order to characterize the distribution of $\{X_n\},$ we must specify $\mathbb{P}\{X_0 = S_0, \dots, X_n = S_n\}$ for all $n \in \bbN$ and for all $S_0, \dots, S_n \in S.$ It is much easier work with conditional probability. We will see that with Markov Chains, the Markov Property guarantees that we only need to worry about the distribution of $X_{n-1}$ to figure out $X_n.$
\end{rem}
\begin{defn}
    If $E,F$ are events with $\mathbb{P}\{F\} >0,$ then the \textbf{conditional probability} of $E$ given $F$ is 
    \[\mathbb{P}(E | F) = \frac{\mathbb{P}(E \cap F)}{\mathbb{P}(F)}\]
\end{defn}
\begin{prop}
    (Law of Total Probability) Recall that if $(B_n) \in S$ is a sequence of mutually exclusive and exhaustive events, then 
    \[P(A) = \sum_{n=1}^\infty P(A \cap B_n) = \sum_{n=1}^\infty P(A \mid B_n)P(B_n)\]
\end{prop}

\begin{defn}
    A stochastic process is called a \textbf{Markov Chain} if for all $n \in \bbN,$ and for all $S_0, \dots, S_n \in S,$ we have that 
    \[\bbP(X_n = S_n |  X_0= S_0, \dots, X_{n-1}= S_{n-1}) = \bbP(X_n = S_n | X_{n-1} =S_{n-1})\]
\end{defn}
\begin{defn}
    A Markov Chain $\{X_n\}$ is \textbf{time-homogeneous} if for all $n\in N,$ for all $x,y \in S,$ 
    \[\bbP(X_n = y | X_{n-1} = x) = \bbP(X_1 = y | X_0 = x)\]
\end{defn}
Thus, it does not matter when you get to the states, and so we can just specify the distribution of $X_0$ and the transition probability;
\[p(x,y) := \bbP(X_1 = y | X_0 = x)\]
and then scale to find the rest.
\begin{exmp}
    Let $S = \{0,1\}.$ A Markov chain taking values in $S$ specified by $p = p(0,1)$ and $q = p(1,0).$ It is obvious that $p(0,0) = 1-p$ and $q(1,1) = 1-q.$
\end{exmp}
\begin{exmp}
    Let $S = \bbZ.$ Let $\{X_n\}_{n\geq 0}$ be defined by $X_0 = X_1 = X_2 =0$ and for $n\geq 3,$ then 
    \[X_n = \begin{cases}
        X_{n-1} + 1,\\
        X_{n-1} - 1,\\
        X_{n-3}
    \end{cases}\]
    each with with probability $1/3.$ This is NOT a Markov Process since $X_n$ can depend on $X_{n-3}.$
\end{exmp}

\begin{defn}
    The $n-$step transition probabilities 
    \[p^n(x,y) = \bbP(X_n = y | X_0 = x).\]
\end{defn}
That is, if we start at $x,$ what is the probability that the $n$th step is $y$?
\begin{prop}
    For all $m,n \in \bbN$ and for all $x,y \in S,$ then 
    \[p^{n + m}(x,y) = \sum_{z\in S}p^n(x,z)p^m(z,y)\]
\end{prop}
\begin{proof}
    We have that by time homogeneity, 
    \[p^n(x,z)p^m(z,y) = \bbP\{X_n = z \mid X_0 = x\} \bbP\{X_{n + m} = y \mid X_n = z\}\] By the Markov Property,
    \[\bbP\{X_{n + m} = y \mid X_n = z\} = \bbP\{X_{n + m} = y \mid X_n = z, X_0 = x\}\] By Bayes' rule:
    \[\bbP\{X_n = z \mid X_0 = x\}\bbP\{X_{n + m} = y \mid X_n = z, X_0 = x\} = \bbP\{X_{n +m} = y, X_n = z \mid X_0 = x\}\] Thus, 
    \[\sum_{z \in S}p^n(x,z)p^m(z,y) = \sum_{z\in S}\bbP\{X_{n +m} = y, X_n = z \mid X_0 = x\} = \bbP\{X_{n + m} = y \mid X_0 = x\} = p^{n + m}(x,y)\]
\end{proof}
\begin{defn}
    The \textbf{transition matrix} of a Markov Chain the the $N \times N$ matrix $P$ whose $ij$ entry is $p(i,j).$
\end{defn}
\begin{prop}
    For each $n \in \bbN,$ $P^n$ is the matrix whose $ij$ entry is $p^n(i,j).$
\end{prop}
\begin{proof}
    We prove by induction. It is trivial for $n = 1.$ Assume it is true for any general $n-1.$ Thus, the $ij$ entry of $P^n = P^{n-1}P$ is \[\sum_{k=1}^n P^{n-1}_{ik}P_{kj} = \sum_{k=1}^n p^{n-1}(i,k)p(k,j) = p^n(i,j),\] where the last equality comes from Proposition 1.
\end{proof}

\newpage
\subsection{Wednesday, Mar 26: Recurrence and Transience for Finite State Space}
We illustrate the stuff from last class with a simple example:
\begin{exmp}
    Consider the two state Markov chain with $S = \{0,1\}$ and 
    \[p(1,0) = \frac{1}{3}, \quad p(1,0) = \frac{1}{2},\] then 
    \[P = \begin{pmatrix}
        p(0,0) & p(0,1)\\
        p(1,0) & p(1,1) 
    \end{pmatrix} = \begin{pmatrix}
        \frac{2}{3} & \frac{1}{3}\\
        \frac{1}{2} & \frac{1}{2}
    \end{pmatrix},\] then 
    \[\bbP\{X_3 = 0 | X_0 = 0\} = p^3(0,0) = \frac{65}{108}\]
\end{exmp}
For the following, we consider a Markov chain $\{X_n\}$ with state space $S.$
\begin{defn}
    Two states $x,y \in S$ \textbf{communicate} if there exists $m,n >0$ such that $p^n(x,y) >0$ and $p^m(y,x) > 0 .$ We write $x \leftrightarrow y.$
\end{defn}
\begin{rem}
    Communication is an equivalence relation, and so we can partition $S$ into a disjoint union of communication classes by modding out the communication classes.
\end{rem}
\begin{defn}
    A communication class $c$ is \textbf{recurrent} if $p(x,y) = 0$ for all $x\in C$ and $y \in S\setminus\{C\}.$ Otherwise, we say that the communication class is \textbf{transient}.
\end{defn}
In other words, if $C$ is recurrent, then the chain never leaves. If it is transcient, then there is no problem leaving.


\begin{defn}
    A Markov chain is \textbf{irreducible} if there is only one communication class.
\end{defn}

\begin{exmp}
    Consider a Markov chain with $S  = \{1,2,3,4,5\}$ and 
    \[P = \begin{pmatrix}
        \frac{1}{5} & \frac{1}{5} & 0 & 0 & \frac{3}{5}\\
        0 & 0 & 0 & 0 & 0 \\
        0 & 0 & \frac{1}{2} & \frac{1}{2} & 0\\
        0 & 0 & \frac{1}{4} & \frac{3}{4} & 0\\
        \frac{1}{2} & \frac{1}{4} & \frac{1}{4} & 0 & 0
    \end{pmatrix}\] then 
    \[C_1 = \{1,2,5\}, \qquad C_2 = \{3,4\}\] and $C_1$ is transient and $C_2$ is recurrent
\end{exmp}

\begin{exmp} (Gambler's ruin)
    Consider the random walk on $S = \{0,1,2, \dots, N\}$ with absorbing boundary, and transition probability
    \[p(x,x+1) = p(x, x-1) = \frac{1}{2}, \quad x\in \{1,2,\dots, N-1\}\] and 
    \[p(0,0) = p(N,N) = 1\]

    $\{1,\dots, N-1\}$ is a transient communication class, while $\{0\}, \{N\}$ are both recurrent communication classes.
\end{exmp}

\begin{prop}
    Suppose $S$ is finite. If $C$ is a recurrent communication class, then if $\{X_n\}$ starts in $C,$ with probability $1,$ $\{X_n\}$ visits every state in $C$ infinitely often. That is, for each $x,y \in C,$ $\bbP\{X_n = y \text{ i.o.} \; | \; X_0 = x\} = 1.$
\end{prop}
\begin{proof}
    Since $C$ is a communication class, then for every $z \in C,$ there exists some $n_z \in \bbN$ such that $p^{n_z}(z,y) >0.$ Let $n = \max\{n_z\; \forall z \in C\},$ and $q = \min\{p^{n_z}(z,y)\; | \; z\in C\}.$ Note that this quantities necessarily exist because $S$ is finite. Let $E_k = \{X_i = y, \; | \; i \in \{(k-1)n + 1, (k-1)n + 2\, \dots (k-1)nk\}\}.$ Then for states $s_0, s_1, \dots, s_{nk} \in S,$ we have that 
    \[\bbP\{E_{k+1} \: | \: X_0 = s_0, X_1 = s_1, \dots, X_{nk} = S_{nk}\} = \bbP\{E_1 \; | \; X_0 = s_{nk}\} \geq q.\] For $M, N \in \bbN$ with $M > N,$ we have that 
    \begin{align*}
      \bbP\{E_k \text{ does not occur for any any } k \in \{N, N + 1, \dots, M\}\} &= \bbP\{\bigcap_{k=N}^M E_k^c\}  \\
      &= \bbP\{E_M^c \:  | \: \bigcap_{k = N}^{M-1}E_k\}\bbP\{\bigcap_{K=N}^{M-1}E_k\}\\
      &\leq (1-q)\bbP\{\bigcap_{K=N}^{M-1}E_k\} \\
      &\leq\dots\leq (1-q)^{M-N} \to 0
    \end{align*}
\end{proof}
\begin{prop}
    Suppose $S$ is finite. If $C$ is a transient communication class, then w.p. 1, $X$ eventually leaves $C$ and never returns.
\end{prop}

\newpage
\subsection{Friday, Mar 28: The Strong Markov Property}

\begin{defn}
    A random time $\tau \in \bbN_0 \cup \{\infty\}$ is called a \textbf{stopping time} if, for all $n \in \bbN,$ the event $\{\tau = n\}$ is determined by $X_0, X_1, \dots, X_n.$ 
\end{defn}
\begin{exmp}
\begin{enumerate}
    \item (The Hitting Time) \[\tau = \min\{n \mid X_n = x\}, \quad x\in S.\]
    In the Gambler's ruin model, where the gambler starts with $k-$dollars and gambles all the way to $N$ or $0$ dollars, the hitting time is the first the the gambler reaches $\$1$.
    \item $\tau = k^\text{th}$ time for which $X_n \in A,$ $k\in \bbN,$ $A \subset S.$
    \item $\tau = \min\{\tau_1, \tau_2\},$ where $\tau_1$ and $\tau_2$ are stopping times. 
    \item Let $N \in \bbN,$ $x\in S,$ $\tau$ be the last time $n \leq N$ for which $X_n = x,$ and $\tau  = 0$ if no such time exists. $\tau$ is NOT a stopping time because it  depends on stuff in the future.
\end{enumerate}
\end{exmp}

\begin{thm}
    (Strong Markov Property) Let $\tau$ be a stopping time for $\{X_n\}.$ Let $n \geq 0,$ $m\geq 1,$ $x_n \dots, x_n \in S$ such that 
    \[\bbP\{X_0 = x_0, \dots, X_\tau = X_n\} >0,\] and 
    let $y_1, \dots, y_m \in S.$ Then 
    \[\bbP\{X_{\tau + 1} = y, \dots, X_{\tau + m} = y_m\mid X_0 = x_0, \dots, X-\tau = x_n\} =\bbP\{X_{\tau + 1} = y, \dots, X_{\tau + m} = y_m\mid X_0 = x_n\}\]
\end{thm}
\begin{proof}
    The event $\{X_0 = x_0, \dots, X_\tau = x_n\}$ is the same as the event $\{X_0 = x_0, \dots, X_n = x_n\}$ and $\{\tau = n\}.$ Since $\tau$ is a stopping time determined where the event $\{\tau = n\}$ by $\{X_0, \dots, X_n\}.$ Thus, we get that the events $\{X_0 = x_0, \dots, X_\tau = x_n\} = \{X_0 = x_0, \dots, X_n = n\}\cap \{\tau = n\},$ then 
    \begin{align*}
        \bbP\{X_{\tau + 1} = y_1, \dots, X_{\tau  + m} = y_m \mid X_0 = x_0, \dots, X_\tau = x_n\} &= \bbP\{X_{n + 1} = y_1, \dots, X_{n  + m} = y_m \mid X_0 = x_0, \dots, X_n = x_n\}\\
        &= \bbP\{X_{n + 1} = y_1, \dots, X_{n  + m} = y_m \mid X_n = x_n\}
    \end{align*} and conclude with the regular markov property and $n-$step invariance.
\end{proof}

\begin{exmp}
    Let $x\in S$ with $S$ finite and let $\tau = \min\{n \geq 0 \mid X_n = x\}.$ Assume that $\bbP\{\tau < \infty\} = 1.$ For any $x_0, \dotss, x_n \in S$ such that 
    $\bbP\{X_0 = x_0, \dots, X_\tau = x_n\} >0,$ we have $x_n = x,$ and so the conditional distribution of $\{X_{\tau + j}\}_{j \geq 0}$ given everything up to $\tau$ is the same as the original chain starting at $x.$ 

    Thus, the Strong Markov property essentially guarantees that the Markov chain resets after hitting $\tau$!
\end{exmp}
    
\begin{defn}
    Let $X_n$ be a Markov chain on a countable state space $S.$ For any $x\in S,$ the \textbf{period} of $x$ is the greatest common divisor of $J_x = \{n \geq 1 \mid p^n(x,x) >0\}.$
\end{defn}
\begin{exmp}
    If $p(x,x) >0,$ then $1 \in J_x,$ and so the period of $x$ is $1.$
\end{exmp}

\begin{thm}
    If $x\leftrightarrow y,$ the periods of $x$ and $y,$ denoted by $d_x$ and $d_y$ respectively, are the same. 
\end{thm}
\begin{proof}
    Choose $n, m$ such that $p^n(x,y) >0$ and $p^m(y,x) >0.$ Then $p^{n +m}(x,x) >0$ and $p^{m + n}(y,y) >0.$ Thus, $ n + m \in J_x \cap J_y.$ Assume that $d_x < d_y.$ Then there exists some $k \in K_x$ not divisible by $d_y.$ Then $n + m + k \in J_y,$ and so $d_y$ divides $n + m + k$ and $n + m,$ and so $d_y$ divides $k,$ which is a contradiction.    
\end{proof}

\begin{defn}
    A Markov chain is aperiodic if every state has period $1.$
\end{defn}

\begin{exmp}
    A knight on an $8\times 8$ chessboard selects one of the next legal moves with equal probability, independently of the past. 
    \[S = \{(x,y) \in \bbR^2 \mid x \in \{1, \dots, 8\}, y \in \{1, \dots, 8\}\}.\]  The period is $2.$ 
\end{exmp}

\newpage
\subsection{Monday, Mar 31: Stationary Distributions}
\begin{defn}
    Let $\tilde{v} = (\tilde{v}_1, \tilde{v}_2, \dots, \tilde{v}_n)$ be a vector such that $\tilde{v}_j = \bbP\{X_0 = j\}.$ We say that $\tilde{v}$ is the \textbf{initial distribution} of the Markov chain.
\end{defn}
\begin{prop}
    For each $i \in [n],$ then $i$th entry of the row vector $\tilde{v} P$ is $\bbP\{X_1 = i\}.$
\end{prop}
\begin{exmp}
    Consider the Markov chain with $S = \{1,2,3,\}$ and 
    \[P = \begin{pmatrix}
        \frac{1}{2} & \frac{1}{4} & \frac{1}{4}\\
        \frac{1}{4} & \frac{1}{2} & \frac{1}{4}\\
        0 & \frac{1}{4} & \frac{3}{4}
    \end{pmatrix}\]
    For $n$ large, 
        \[P^n = \begin{pmatrix}
        \frac{1}{6} & \frac{1}{3} & \frac{1}{2}\\
        \frac{1}{6} & \frac{1}{3} & \frac{1}{2}\\
        \frac{1}{6} & \frac{1}{3} & \frac{1}{2}
    \end{pmatrix}.\] We encounter the phenomena that the limit of $P^n,$ if it exists, has identical rows. 

    Call this row $\pi.$ For any probability vector $\Tilde{v},$ $\lim_{n\to \infty} \Tilde{v} P^n = \pi$
\end{exmp}

Suppose $\pi$ is a limiting probability vector. That is, for any initial distribution $\Tilde{v},$ $\lim_{n\to \infty}\Tilde{v}P^n = \pi.$ Then 
\[\pi = \lim_{n\to \infty}\Tilde{v}P^{n + 1} = (\lim_{n\to \infty}\Tilde{v}P^n)P = \pi P\] We say that $\pi$ is a {stationary/invariant/equilibrium/steady-state} distribution for the Markov chain. 

\begin{defn}
    Let $\pi: S \to [0,1]$ be a probability distribution on $S$ such that $\sum_{x\in S} \pi_x = 1.$ We say that $\pi$ is a \textbf{stationary distribution} of $\{X_n\}$ if 
    \[\pi_y = \sum_{x\in S}\pi_x p(x,y), \quad \forall y \in S.\] That is, $\pi P =  \pi.$
\end{defn}

\begin{thm}
    If $S$ is finite and $\{X_n\}$ is an irreducible and aperiodic chain, then there exists a unique stationary distribution $\pi$ for $\{X_n\}.$ Moreover, for any $x,y \in S,$ \[\lim_{n\to \infty}p^n(y,x) = \pi_y\]
\end{thm}

\begin{proof}
    (Existence) Fix $z \in S.$ Suppose $X_0 = z.$ Let $\tau = \min\{n \geq 1 \mid X_n = z\}$ be the first return time to $z.$ Note that $\tau < \infty $ by proposition $4.$ For any $x\in S,$ define
    \[\Tilde{\pi}_x := \mathbb{E}[ \#\{n \in \{0,1, \dots \tau - 1\} \mid X_n = x\}]\] 
    We claim that 
    \[x \mapsto \frac{\pi_x}{\bbE[\tau]}\] is a stationary distribution for $\{X_n\}.$ We want to show that for all $y \in S,$ $\tilde{\pi}_y = \sum_{x\in S} \Tilde{\pi}_x p(x,y).$ Evidently, 
    \[\tilde{\pi}_x = \mathbb{E}\left[\sum_{x\in S}\mathbbm{1}_{\{X_n = x\}}\right] = \mathbb{E} \left[\sum_{n = 0}^\infty \mathbbm{1}_{\{X_n = x, \tau >n\}}\right] = \sum_{n =0}^\infty \bbP\{X_n = x, \tau >n\}.\] Thus, 
    \[\sum_{x\in S}\tilde{\pi}_x p(x,y) = \sum_{x\in S}\sum_{n = 0}^\infty \bbP\{X_n = x, \tau >n\} p(x,y) = \sum_{n = 0}^\infty \sum_{x\in S} \bbP\{X_n = x, T >n X_{n + 1}  = y\} = \sum_{n = 0}^\infty \bbP\{\tau >{n + 1}X_{n + 1} = y\} = \tilde{\pi}_y\]
    In order to find a stationary probability distribution, we need that 
    \[\sum_{x\in S} \frac{\tilde{\pi}_x}{\mathbb{E}[\tau]} = 1.\] But then 
    \[\tilde{pi}_x = \sum_{n = 0}^\infty \bbP\{X_n = x , \tau >n\} = \sum_{n = 0}^\infty \bbP\{\tau > n, X_n = x \mid X_n = x\}.\] So then 
    \[\tilde{\pi}_x  = \sum_{n  = 0}^\infty \sum_{x\in S}\bbP\{\tau > n  \mid X_n = x\}\bbP\{\tau  > n \mid X_n  = x\}\bbP\{X_n = x\} = \sum_{n = 0}^\infty \bbP\{\tau >n\} = \bbE[\tau]\]
\end{proof}

\begin{exmp}
    Consider a Markov chain $\{X_n\}$ with 
    \[P = \begin{pmatrix}
        \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
        \frac{1}{2} & 0 & \frac{1}{2}\\
        \frac{1}{5} & \frac{1}{5} & \frac{3}{5}
    \end{pmatrix}.\] $\{X_n\}$ is clearly aperiodic and irreducible. To compute $\pi,$ we solve the system $\pi P  = \pi,$ with $\pi_1 + \pi_2  + \pi_3 = 1.$ 
    \[(\pi_1 , \pi_2 ,\pi_3) P  = \pi \iff \frac{1}{3}\pi_1 + \frac{1}{2}\pi_2 + \frac{1}{5}\pi_3 = \pi_1,\quad  \frac{1}{3}\pi_1 + \frac{1}{5}\pi_3  = \pi_2, \quad  \frac{1}{3}\pi_1 + \frac{1}{2}\pi_2 + \frac{3}{5}\pi_3.\] Alternatively, we know that 
    \[\pi P = \pi \iff P^T \pi^T=\pi,\] and so we solve for the eigenvectors of $P^T$ corresponding to $\lambda = 1$ and then normalizing so that $\pi_1 + \pi_2 + \pi_3 = 1.$
    Solving, 
    \[\pi = (\frac{3}{10}, \frac{1}{5}, \frac{1}{2})\]
\end{exmp}


\newpage
\subsection{Wednesday, Apr 2: Uniqueness of Stationary Distributions}
\begin{thm}
    If $\pi$ is a stationary distribution for $\{X_n\},$ then for any $x,y \in S,$  then $\lim_{n\to \infty}\{\bbP\{X_n = y \mid X_0= x\}\} = \pi_y.$ Then $\pi$ is unique.
\end{thm}
\begin{proof}
    Let $\{X_n\}$ and $\{Y_n\}$ be two Markov chains starting at $x$ and $y,$ respectively. Consider the Markov chain $(X_n, Y_n)$ in $S\times S$ with transition probability \[\overline{p}((x,y), (x',y')) = \begin{cases}
        p(x,x')p(y,y'), \quad x\neq y\\
        p(x,x'), \qquad \qquad x' = y'\\
        0, \qquad \qquad \qquad \;\;x' \neq y'
    \end{cases}\]
    Note that $\bbP\{X_1 = x' \mid X_0 = x, Y_0 = y\} = \sum_{y \in S}p(x,x')p(y,y') = p(x,x').$ Then $\{X_n\}$ and $\{Y_n\}$ have the same as our original Markov chain. Let 
    \[\tau = \min\{n \mid X_n = Y_n\}.\] We claim that $\bbP\{\tau < \infty \mid X_0 = x, Y_0 = y\} = 1.$ We prove this on the homework. 

    Consider $(X_n, Y_n)$ where where $X_0 = x$ and $Y_0 = \pi,$ then $Y_n$ has distance $\pi$ for all $n,$ and for large $n,$ $X_n = Y_n.$ FOr any $y,$ 
    \[\lim_{n\to \infty}\bbP\{X_n = y \mid X_0 = x\} -\pi_ y = \lim_{n\to \infty}\left(\bbP\{X_n = y\} - \bbP\{Y_n - y\}\right) = 0.\]  Thus, if $\pi, \tilde{\pi}$ are two stationary distributions, then 
    \[\pi_y = \lim_{n\to \infty}\bbP\{X_n = y \mid X_0 = x\}= \tilde{\pi}_y\]
\end{proof}

\begin{prop}
    Let $\pi$ be a stationary distribution for $\{X_n\}$ and let $T_x:= \min\{n \geq 1 \mid X_n = x\}.$ Then 
    \[\pi_x = \frac{1}{\bbE[T_x \mid X_0 = x]}\]
\end{prop}
\begin{proof}
    Assume $X_0 =x.$ Then the stationary distribution is given by
    \[\pi_y = \frac{1}{\bbE[T_x]}\bbE[|\{n \in [T_x - 1] \mid X_n = y|].\] By definition, 
    \[|n \in \{0,1, \dots, T_x -1\}\mid X_n = x| = 1\] so then we are done.
\end{proof}

\begin{exmp}
    Let $G = (V,E)$ be a finite connected, non-bipartite graph. Then $\{X_n\}$ is irreducible and aperiodic. It is not hard to show that the sum of all the degrees is $2|E|$ since you need to count each edge twice. The stationary distribution for $\{X_n\}$ is 
    \[\pi_X = \frac{\deg x}{2|E|}.\] Note that $\pi$ is indeed a valid probability distribution since
    \[\sum_{x\in V} \pi_x = \sum_{x\in V}\frac{\deg x}{2 |E|} = \frac{2 |E|}{2 |E|} = 1.\] If $p(x,y) = \frac{1}{\deg x}$ and $x\sim y$ when they are joined by an edge and $0$ else, then for any $y \in V,$ 
    \[\sum_{x \in V}\pi_x p(x,y) = \sum_{x \mid x\sim y} \frac{1}{\deg x}\frac{\deg x}{ 2  |E|} = \frac{\deg y}{2 |E|} = \pi_y\]
\end{exmp}

\begin{exmp}
    Consider the Knight's Tour, where a knight chooses a legal move randomly on an $8 \times 8$ chessboard. What is expected time to return to the bottom left corner. The degrees are given by:
    \[\begin{bmatrix}
        2 & 3 & 4 & 4 & 4 & 4& 3 & 2\\
        3 & 4 & 6 & 6 & 6 & 6 & 4 & 3\\
        4 & 6 & 8 & 8 & 8 & 8 & 6 & 4\\
        \vdots\\
        2 & 3 & 4 & 4 & 4 & 4 & 3 & 2
    \end{bmatrix}\]
    Where the sum of degrees is $336,$ and so the expected time to return to $(1,1)$ is $\frac{336}{2}= 168.$
\end{exmp}

\begin{exmp}
    (King's) Tour
    \begin{exmp}
        \[\begin{bmatrix}
            3 & 5 & 5 & 5 & 5 & 5 & 5 & 3\\
            5 & 8 & 8 & 8 & 8 & 8 &8 &5\\
            \vdots\\
            3 & 5 & 5 & 5 & 5 & 5 & 5 & 3
        \end{bmatrix}\]
    \end{exmp}
    So then the expected number of moves till return is $140.$
\end{exmp}

\newpage
\subsection{Friday, Apr 4: First Step Analysis}
Our goal is to calculate the expected duration (that is, the hitting probability) conditioned on a first step. Let's illustrate using an example! Before, we do a little aside to talk about a useful technique that comes up naturally:
\begin{rem}
    Suppose $f$ satisfies
    \[f(n) = a f(n-1)+ bf(n+1), \qquad 0 \leq n \leq N,\] where $f(0)$ and $f(N)$ are known. Guess the solution to be 
    \[f(n) = \alpha^n,\] then 
    \[\alpha^n = a\alpha^{n-1} + b\alpha^{n+1} \implies \alpha = a + b\alpha^{2}\implies \alpha = \frac{1 \pm \sqrt{1 - 4ab}}{2b}.\] Thus the general solution is 
    \[f(n) = \lambda_1 \alpha_+^n + \lambda_2 \alpha_-^n\]
\end{rem}
\begin{exmp}
    Consider the Gambler's Ruin where $S = \{0, 1,2, \dots, N-1, N\}$ with transition probabilities
    \[p(x, x+1) = p, \qquad p(x, x-1) = q, \quad x\notin \{0,N\}\]
    \[p(0,0) = p(N,N) = 1.\]
    That is, we have two absorbing boundaries and so we have three communication classes:
    \[C_1 = \{0\}, \quad C_2 = \{1,2,\dots, N-1\}, \quad C_3 = \{N\}.\] Note that $C_1$ and $C_3$ are recurring and $C_2$ is transient. Define
    \[\tau_k := \min\{n >0 : X_n\in C_1 \cup C_3 \mid X_0 = k\}, \quad P_k = \bbP\{X_{\tau_k} = N\}.\] We defined $P_k$ to be the probability that the gambler wins starting with $k$ dollars. Clearly, $P_0 = 0,$ $P_N = 1.$ Using the law of total probability
    \[P_k  =\bbP\{X_{\tau_k} = N\} = \bbP\{X_{\tau_k}  = N\mid X_1 = k+1\}\} \bbP\{X_1 = k+1\} + \bbP\{X_{\tau_k}  = N\mid X_1 = k-1\}\} \bbP\{X_1 = k-1\} = pP_{k+1} + qP_{k-1}.\]
    Using the above remark we see that 
    \[\alpha_+ = 1, \quad \alpha_- = \frac{1-p}{p}\]
    so 
    \[P_n = \lambda_1 + \lambda_2(\frac{1-p}{p})^n \implies P_0 = \lambda_1 + \lambda_2 = 0, \quad P_1 = \lambda_1 + \lambda_2\frac{1-p}{p} = 1\]
    Solving:
    \[\lambda_1 = \]
\end{exmp}

\newpage

\subsection{Wednesday, Apr 9: Queuing and Stationary Distributions for Countable State Spaces}
\begin{exmp}
    Let's continue the queue example from last class. Let $\{X_n\}$ be the number of people at time $n.$ If $p(0,1) = 0$ and $p(0,0) = 1-p$ and
    \[p(x,x-1) = q(1-p), \quad p(x,x+1) = p(1-q), \quad p(x,x) = pq + (1-q) + (1-p).\] Thus, $p$ is probability a person enters the line and $q$ is the probability a person leaves (or is served). 
    \begin{prop}
        The queue is recurrent if and only if $q \geq p.$ 
    \end{prop}
    \begin{proof}
        Let $\tau_k$ be the $k$th time for which $X_n \neq X_{n-1}.$ For $x \geq 1,$ 
        \begin{align*}
        \bbP\{X_{\tau_k} = x + 1 \mid X_{\tau_{k-1}}= x\} &= \bbP\{X_{\tau_k} = x + 1 \mid X_{\tau_k -1} = x\}\\ &= \bbP\{X_1 = x + 1 \mid X_0 = x, X_1 \neq X_0\}\\ &= \frac{p(x,x+1)}{1 - p(x,x)}\\ &= \frac{p(1-q)}{q(1-p) + p(1-q)}    
        \end{align*}
        and by the same logic, 
        \[\bbP\{X_{\tau_k} = x-1 \mid X_{\tau_{k-1}} = x\} = 1- \frac{p(1-q)}{q(1-p) + p(1-q)}\]
        Thus, $\{X_{\tau_k}\}_{k\geq 0}$ is a biased random walk with parameter $\frac{p(1-q)}{q(1-p) + p(1-q)}.$ Thus, $\{X_{\tau_k}\}$ hits $0$ with probability 1 if and only if $\{X_n\}$ started from $x\geq 1$ hit $0$ with probability $1.$ From last class, happens if and only if 
        \[\frac{p(1-q)}{q(1-p) + p(1-q)} \leq \frac{1}{2} \iff p(1-q) \leq q(1-p) \iff p \leq q\]
    \end{proof}
\end{exmp}

\begin{rem}
    By the Brower fixed point theorem, we have the existence of a stationary distribution in finite space spaces. 
\end{rem}

Recall that if $S$ is finite, then for an irreducible, aperiodic Markov chain, there exists a unique stationary distribution $\pi$ that satisfies 
\[\sum_{x\ in S} \pi_x = 1 , \qquad \pi P = \pi \iff \sum_{x\in S} \pi_x p(x,y) = \pi_y.\]

In the countably infinite space, we note that a transient state space cannot yield a stationary distribution since the expected return times are all $\infty.$
\begin{defn}
    Suppose $\{X_n\}$ is irreducible and recurrent. We say that $\{X_n\}$ is \textbf{null recurrent} if 
    \[\lim_{n\to \infty}p^n(x,y)= 0, \quad \forall\; x,y \in S.\] Otherwise, $\{X_n\}$ is \textbf{positive recurrent}.
\end{defn}
\begin{rem}
    Clearly, null recurrent Markov chains also have infinite return times, and thus have no stationary distribution.
\end{rem}
\begin{prop}
    Suppose $\{X_n\}$ is irreducible. The following are equivalent:
    \begin{enumerate}
        \item $\{X_n\}$ is positive recurrent;
        \item $\{X_n\}$ has a stationary distribution;
        \item For any $x,y \in S,$ 
        \[\limsup_{n\to \infty} p^n(x,y) > 0.\]
        \item If $T_x = \min\{n \geq 1 \mid X_n = x\},$ then 
        \[\bbE[T_x \mid X_0 = x] < \infty\] for any $x \in S.$
    \end{enumerate}
    Furthermore, if $\{X_n\}$ is aperiodic and positive recurrent, then $\pi$ is unique and for any $x\in S,$
    \[\pi_x = \frac{1}{\bbE[T_x \mid X_0 = x]}\]
\end{prop}

\begin{rem}
    By condition 3, checking that a recurrent Markov chain is null recurrent amount to checking that $\lim_{n\to \infty}p^n(x,y) = 0$ for some states $x,y \in S.$
\end{rem}
\begin{exmp}
    Consider the biased random walk on $\{0,1, \dots\}$ with partially reflecting boundary: 
    \[p(x,x-1) = q= 1-p, \quad p(x,x+1) = p, \quad p(0,0) = q, \quad p(0,1) =p.\]
    Is this positive recurrent?

    We know that $\{X_n\}$ is transient for $p > \frac{1}{2}.$ For some $\pi$ to possibly exist, we need $p < \frac{1}{2}.$ The stationary distribution must satisfy:
    \[\pi(x) = p\pi(x-1) + q\pi(x+1), \quad \forall x\geq 1.\] Moreover, 
    \[\pi(0) = q\pi(0) + q\pi(1).\] This is because 
    \[\pi P = \pi \begin{pmatrix}
        q & p & 0 & 0 & \cdots\\
        q & 0 & p & 0 & \cdots \\
        0 & q & 0 & p& \cdots\\
        \vdots&\vdots& \vdots & \vdots &\ddots
    \end{pmatrix} = \begin{pmatrix}
        \pi_0\\ \pi_1 \\
        \vdots
    \end{pmatrix}\] For $p < \frac{1}{2},$ the solution takes the form 
    \[\pi(x) = \lambda_1 + \lambda_2 (\frac{p}{1-p})^x\] We need 
    \[\sum_{x = 0}^\infty \pi(x) = 1 \implies \lambda_1 = 0.\] After some algebra, we find that $\lambda_1 = 1- \frac{p}{1-p},$ and thus 
    \[\pi(x) = (1- \frac{p}{1-p})(\frac{p}{1-p})^x.\] Thus, for $p< \frac{1}{2},$ $\{X_n\}$ is positive recurrent. 
    
    For $p = \frac{1}{2},$ the general solution is 
    \[\pi(x) = \lambda_1 + \lambda_2 x.\] But
    \[\sum_{n=0}^\infty \pi(x) < \infty \implies \lambda_1 = \lambda_2 = 0,\] and thus $\{X_n\}$ is null recurrent. 
\end{exmp}




















\newpage
\section{Problem Sessions}
\subsection{Monday, Mar 31: Problem Session 1}
When do we run into the issue that $\lim_{n\to \infty}\pi P^n$ does not exist? 
\begin{enumerate}
    \item[(1)] If Period $> 1$
    \item[(2)] If there are multiple recurrence classes that are transient.
\end{enumerate}

Problems:
\begin{enumerate}
    \item Let $X_0, X_1, \dots$ be a Markov chain with state space $S = \{0, 1, 2, 3\}$, and with transition matrix
    \[
    \begin{bmatrix}
        \frac{1}{4} & 0 & \frac{1}{2} & \frac{1}{4} \\
        0 &  \frac{1}{5} & 0 & \frac{1}{5} \\
        0 & 1 & 0 & 0 \\
        \frac{1}{3} & \frac{1}{3} & 0 & \frac{1}{3}
    \end{bmatrix}.
    \]
    A new process is defined by $Z_n = 0$ if $X_n = 0$ or $1$ and $Z_n = X_n$ if $X_n = 2$ or $3$. Find $P(Z_{n+1} = 2 | Z_n = 0, Z_{n-1} = 2)$ and $P(Z_{n+1} = 2 | Z_n = 0, Z_{n-1} = 3)$. Is $Z_n$ a Markov chain?
    \begin{solution}
    \begin{align*}
        \bbP\{Z_{n + 1}= 2 \mid Z_n = 0, Z_{n-1} = 2\} &= \frac{\bbP\{Z_{n + 1} = 2, Z_{n} = 0, Z_{n-1} = 2\}}{\bbP\{Z_n = 0, Z_{n-1 } = 2\}}\\
        &= \frac{\bbP\{Z_{n + 1} = 2, Z_{n} = 0 \mid Z_{n-1} =2\}}{\bbP\{Z_n = 0 \mid Z_{n-1} = 2\}}\\
        &= 0
    \end{align*}
    \begin{align*}
        \bbP\{Z_{n + 1} = 2 \mid Z_n = 0, Z_{n-1} = 3\} &= \frac{\bbP\{Z_{n + 1} = 2, Z_{n} = 0 \mid Z_{n-1} =3\}}{\bbP\{Z_n = 0 \mid Z_{n-1} = 3\}}\\
        &= \frac{\bbP\{X_n = 2, X_n = 0 \mid X_{n-1} = 3\} +\bbP\{X_n = 2, X_n = 1 \mid X_{n-1} = 3\}}{\bbP\{X_n = 0 \mid X_{n-1} =3\} + \bbP\{X_{n} = 1 \mid X_{n-1} = 3\}}\\
        &= \frac{\frac{1}{3}\frac{1}{2} + 0}{\frac{1}{6} + \frac{1}{6}}\\
        &= \frac{1}{4}
    \end{align*}
    Thus, $\{Z_n\}$ is not a Markov chain
    \end{solution}

    \item We repeatedly roll two four-sided dice with numbers 1, 2, 3, and 4 on them. Let $Y_k$ be the sum on the $k$-th roll, $S_n = Y_1 + Y_2 + \dots + Y_n$ be the total of the first $n$ rolls, and $X_n = S_n (\text{ mod } 6)$. Find the transition matrix for $\{X_n\}$.
    \begin{solution}
        $\{X_n\}$ is a Markov process because it only depends on the current state $S_n,$ as you can figure out the next turn only from this, since 
        \[X_{n + 1} = S_{n + 1}\text{ mod } 6 = (S_{n} + Y_{n + 1})\text{ mod } 6= S_n \text{ mod } 6 + Y_{n + 1}\text{ mod } 6 = X_n + Y_{n+1}\text{ mod } 6\] 
        \[P = \begin{bmatrix}
            \frac{3}{16} & \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{3}{16} & \frac{1}{4}\\
             \frac{1}{4} & \frac{3}{16}& \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{3}{16} \\
             \frac{3}{16} & \frac{1}{4} & \frac{3}{16}& \frac{1}{8} & \frac{1}{8} & \frac{1}{8} \\
             \frac{1}{8} & \frac{3}{16} & \frac{1}{4} & \frac{3}{16}& \frac{1}{8} & \frac{1}{8} \\
             \frac{1}{8} &\frac{1}{8} & \frac{3}{16} & \frac{1}{4} & \frac{3}{16}& \frac{1}{8} \\
             \frac{1}{8} &\frac{1}{8} &\frac{1}{8} & \frac{3}{16} & \frac{1}{4} & \frac{3}{16} 
        \end{bmatrix}\]
    \end{solution}

    \item Consider a Markov chain with states $S = \{0, \dots, N\}$ and transition probabilities $p(i, i+1) = p$, $p(i, i-1) = q$, for $1 \leq i \leq N-1$, where $p+q = 1$, $0 < p < 1$. Assume $p(0,1) = p(N, N-1) = 1$.
    \begin{enumerate}
        \item Draw a transition diagram for this chain.
        \begin{solution}
        In the following diagram, if you the probability  is above, then it is going to the right:
            \begin{center}
\begin{tikzpicture}[scale=1.2, every node/.style={draw, circle, inner sep=1pt}]
    \node (A) at (-4,0) {0};
    \node (B) at (-2,0) {1};
    \node (C) at (0,0) {$\cdots$}
    \node (D) at (2,0) {$N-1$};
    \node (E) at (4, 0) {$N$};

    \draw[->] (A) -- (B) node[midway, above] {\textcolor{red}{$1$}};
    \draw[->] (B) -- (A) node[midway, below] {\textcolor{red}{$q$}};
    \draw[->] (B) -- (C) node[midway, above] {\textcolor{red}{$p$}};
    \draw[->] (C) -- (B) node[midway, below] {\textcolor{red}{$q$}};
    \draw[->] (C) -- (D) node[midway, above] {\textcolor{red}{$p$}};
    \draw[->] (D) -- (C) node[midway, below] {\textcolor{red}{$q$}};
    \draw[->] (D) -- (E) node[midway, above] {\textcolor{red}{$p$}};
    \draw[->] (E) -- (D) node[midway, below] {\textcolor{red}{$1$}};
\end{tikzpicture}
\end{center}            
        \end{solution}
        \item Is the Markov chain irreducible? 
        \begin{solution}
Let $i,j \in [N],$ then we claim that $i \leftrightarrow j.$ Without loss of generality, suppose that $i < j.$ Suppose $j-i = N.$ Then $p(i,j)\geq p^N >0$ and $p(j,i) \geq q^N >0.$
        \end{solution}
        \item What is the period of this chain? 
        \begin{solution}
            The period has to be $2.$
        \end{solution}
    \end{enumerate}

    \item A taxicab driver moves between the airport $A$ and two hotels $B$ and $C$ according to the following rules. If he is at the airport, he will be at one of the two hotels next with equal probability. If at a hotel, then he returns to the airport with probability $\frac{3}{4}$ and goes to the other hotel with probability $\frac{1}{4}$.
    \begin{enumerate}
        \item Find the transition matrix for the chain.
        \begin{solution}
        \[
            P = \begin{bmatrix}
                0 & \frac{1}{2} & \frac{1}{2}\\
                \frac{3}{4} & 0 & \frac{1}{4}\\
                \frac{3}{4} & \frac{1}{4} & 0
            \end{bmatrix}\]
        \end{solution}
        \item Suppose the driver begins at the airport at time 0. Find the probability for each of his three possible locations at time 2 and the probability he is at hotel $B$ at time 3.
        \begin{solution}
        Squaring the matrix gives
                \[P^2 = \begin{bmatrix}
                    \frac{3}{4} & \frac{1}{8} & \frac{1}{8}\\
                    \frac{3}{16} & \frac{7}{16} & \frac{6}{16}\\
                    \frac{3}{16} & \frac{6}{16} & \frac{7}{16}
                \end{bmatrix}\]
                and so 
                \[p^2(A,A) = \frac{3}{4}, \quad p^2(B,A) = \frac{3}{16}, \quad p^2(C,A) = \frac{3}{16}\]
            Similarly, 
            \[p^3(B,A) = \frac{13}{32}\]
        \end{solution}
    \end{enumerate}

    \item At time $n = 0$, two ladybirds are placed at vertices $i$ and $j$ of a regular hexagon, whose vertices are labeled $1, \dots, 6$. At time $n = 1$, each of them moves, independently of the other, to one of the two adjacent vertices with probability $\frac{1}{2}$, and so on at each time $n = 2, 3, \dots$.
    \begin{enumerate}
        \item Denote $X_n$ the distance between the two ladybirds at time $n \geq 0$, i.e., the minimum number of edges between them. Find the transition matrix for $\{X_n\}$.
        \begin{solution}
            $S = \{0, 1, 2, 3\}$
            \[P = \begin{bmatrix}
                \frac{1}{2} & 0 & \frac{1}{2} & 0\\
                0 & \frac{3}{4} & 0 & \frac{1}{4}\\
                \frac{1}{4}& 0 & \frac{3}{4} & 0\\
                0 & \frac{1}{2} & 0 & \frac{1}{2}
            \end{bmatrix}\]
        \end{solution}
        \item Identify the communication classes. Are they recurrent or transient?
        \[C_1 = \{0,2\}, \quad C_2 = \{1,3\}.\] Both are recurrent.
    \end{enumerate}

    \item Find the invariant (stationary) distributions for the following Markov chains with given transition matrices:
    (a)
    \[\begin{bmatrix}
        \frac{1}{2} & \frac{2}{5} & \frac{1}{10}\\
        \frac{1}{5} & \frac{1}{2} & \frac{3}{10}\\
        \frac{1}{10} & \frac{3}{10} & \frac{3}{5}
    \end{bmatrix}\]
    \begin{solution}
        This will be the only time I type out a full solution to this. 
        We need to solve for the normalized eigenvector of $P^T$ corresponding to $\lambda = 1.$ It suffices to find $\pi^T$ such that 
        \[10 P^T - 10\pi^T = 0\] That is, we need to find the eigen basis of
        \begin{align*}
            E_{10}(10P^T) &= 
            \begin{pmatrix}
            5 & 2 & 1\\
            4 & 5 & 3\\
            1 & 3 & 6
            \end{pmatrix} - 10I\\
        &= \begin{pmatrix}
            -5 & 2 & 1\\
            4 & -5 & 3\\
            1 & 3 & -4
            \end{pmatrix}\\
            &\simeq 
            \begin{pmatrix}
            -20 & 8 & 4\\
            20 & -25 & 15\\
            20 & 60 & -80
            \end{pmatrix}\\
            &\simeq
            \begin{pmatrix}
            -20 & 8 & 4\\
            0 & -17 & 19\\
            0 & 68 & -76
            \end{pmatrix}\\
            &\simeq
            \begin{pmatrix}
            1 & -\frac{2}{5} & - \frac{1}{5}\\
            0 & 1 & -\frac{19}{17}\\
            0 & -68 & 76
            \end{pmatrix}\\
            &\simeq
            \begin{pmatrix}
            1 & -\frac{2}{5} & - \frac{1}{5}\\
            0 & 1 & -\frac{19}{17}\\
            0 & -68 & 76
            \end{pmatrix}\\
            &\simeq
            \begin{pmatrix}
            1 & 0 & - \frac{11}{17}\\
            0 & 1 & -\frac{19}{17}\\
            0 & 0 & 0
            \end{pmatrix}
        \end{align*}
        Thus, 
        \[\pi_1^T = \frac{11}{17}\pi_3^T, \qquad \pi_2^T = \frac{19}{17}\pi_3^T, \qquad \pi_1^T + \pi_2^T + \pi_3^T = 1\] Solving:
        \begin{align*}
            \left[\begin{array}{ccc | c}
               1  & 0 &-\frac{11}{17} & 0 \\
               0  & 1 &-\frac{19}{17} & 0\\
               1 & 1 & 1 & 1
            \end{array}\right] &\simeq
            \left[\begin{array}{ccc | c}
               1  & 0 &-\frac{11}{17} & 0 \\
               0  & 1 &-\frac{19}{17} & 0\\
               0 & 0 & \frac{28}{17} & 1
            \end{array}\right]\\
            &\simeq
            \left[\begin{array}{ccc | c}
               17  & 0 &-11 & 0 \\
               0  & 17 &-19 & 0\\
               0 & 0 & 28 & 47
            \end{array}\right]\\
            &\simeq
            \left[\begin{array}{ccc | c}
               1  & 0 &0 & \frac{11}{47} \\
               0  & 1 &0 & \frac{19}{47 }\\
               0 & 0 & 1 & \frac{17}{47}
            \end{array}\right]
        \end{align*}
        So then 
        \[\pi = (\frac{11}{47}, \frac{19}{47}, \frac{17}{47})\]
    \end{solution}
    (b)
    \[
    \begin{bmatrix}
        \frac{1}{2} & \frac{2}{5} & \frac{1}{10} \\
        \frac{3}{10} & \frac{2}{5} & \frac{3}{10} \\
        \frac{1}{5} & \frac{1}{5} & \frac{3}{5}
    \end{bmatrix}.
    \]
    \begin{solution}
        \pi = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})
    \end{solution}
\end{enumerate}

\newpage
\subsection{Monday, Apr 7: Problem Session 2}
\begin{enumerate}
    \item A queen can move any number of squares horizontally, vertically, or diagonally on an $8 \times 8$ chessboard. Let $\{X_n\}$ be the sequence of squares that results if we pick one of the queen’s legal moves uniformly at random. Find
        \begin{enumerate}
            \item the stationary distribution and
    \begin{solution}
        We use Example 1.12. First, consider that the following 4x4 grid of the degrees for each positions on the upper left of the board:
        \begin{bmatrix}
            21 & 21 & 21 & 21\\
            21 & 23 & 23 & 23\\
            21 & 23 & 25 & 25\\
            21 & 23 & 25 & 27
        \end{bmatrix}
        Thus, 
        \[\pi_{(0,0)} = \frac{\deg (1,1)}{2|E|} = \frac{21}{1456}\]
    \end{solution}
            \item the expected number of moves needed to return to the bottom left corner when we start there.
        \begin{solution}
            If we define 
            \[T_{i,j} = \inf[n : X_n = (i,j) \mid X_0 = (i,j)],\]
            then by Example 1.12, 
            \[\bbE[T_{0,0}] = \frac{1}{\pi_{0,0}} = \frac{1456}{21}\]
        \end{solution}
        \end{enumerate}

    \item Let $\{X_n\}$ be the random walk on $\{-10, -9, \ldots, 9, 10\}$ with reflected boundary, i.e., the Markov chain with transition probabilities
        $$
        p(x, x+1) = p(x, x-1) = \frac{1}{2}, \quad \forall x \in \{-9, \ldots, 9\}, \quad p(-10, -9) = p(10, 9) = 1.
        $$
        Assume that $X_0 = 0$ and let
        $$
        T = \min \{ n \geq 1 : X_n = 1 \}.
        $$
        Find the expected number of times that the walk hits 0 before time $T$, i.e., compute
        $$
        \mathbb{E}\left[ \#\{n \in \{0, \ldots, T\} : X_n = 0\} \right].
        $$
        (Hint: use the strong Markov property applied at the times for which $X_n = 0$).
    \begin{solution}
        Call 
        \[T_1 = \inf\{n  : X_n = 1 \mid X_0 = 0\}\] Call
        \[K(i, j) = \sum_{n=1}^j \mathbbm{1}_{X_n = 1}\]
        Then using the law of total expectation:
        \begin{align*}
            \bbE[K(0, T_1)] &= \bbE[\bbE[K(0,T_1) \mid X_1]]\\
            &= \frac{1}{2}(\bbE[K(0, T_1)\mid X_1 = -1] + 1) + \frac{1}{2}(\bbE[K(0,T_1) \mid X_1 = 1] +1)\\
            &\text{By definition:}\\
            &= \frac{1}{2}(\bbE[K(0, T_1)\mid X_1 = -1] + 1) + \frac{1}{2}(0 +1)\\
            &\text{Strong Markov Property:}\\
            &= \frac{1}{2}(\bbE[K(T_0, T_1)] + 1) + \frac{1}{2}(0 +1)\\
            &= \frac{1}{2} + \frac{1}{2}(\bbE[K(0, T_1)] +1)
        \end{align*}
        Solving:
        \[\bbE[K(0, T_1)] = 2.\] 
    \end{solution}

    \item Consider the numbers $\{1, 2, 3, \ldots, 12\}$ written around a ring as they usually are on a clock. Consider a Markov chain $\{X_n\}$ that at any point jumps with equal probability to the two adjacent numbers.
        \begin{enumerate}
            \item What is the expected number of steps that $\{X_n\}$ will take to return to its starting position?
            \begin{solution}
            Suppose WLOG $X_n = 12.$ Then
                $2|E| = 24$ and $\deg 12 = 2.$ Thus, if 
                \[T_{12} = \inf\{n>0 : X_n = 12 \mid X_0 = 12\},\] then
                \[\bbE[T_{12}] = \frac{1}{\pi_{12}} = \frac{2|E|}{\deg(12)} = \frac{24}{2} = 2.\]
            \end{solution}
            \item What is the probability that $\{X_n\}$ will visit all the other states before returning to its starting position?
            \begin{solution}
                We will use the law of total probability. Again, WLOG, assume $X_0 = 12.$ Define 
                \[E_{12} = \text{running through everything before returning to 12}\]
                \[\bbP\{E_{12}\} = 2(\bbP\{E_{12} \mid X_1 = 1\}\bbP\{X_1 = 1\})\]
                Thus, we can make 
                $\bbP\{E_{12} \mid X_1 = 1\}$ a gambler's ruin problem! 
\begin{figure}[H]
                    \centering
                    \includegraphics[width=0.5\linewidth]{Images/MarkovProof.png}
                    \caption{Gambling With a Clock}
                \end{figure}
                                
                the probability of reaching 12 on the left is given by $1 - P_{11} = 1 - \frac{11}{12} = \frac{1}{12}$
            \end{solution}
        \end{enumerate}

    \item Before you are six light bulbs (all off), numbered $\{1, 2, \ldots, 6\}$. At each time $n \geq 1$, you roll a fair six-sided die and flip the switch of the bulb corresponding to the number you have rolled, turning it on if it is off, and off if it is on. What is the expected number of rolls needed until every light bulb is turned on?
    

    \item Find the probability that, in the process of repeatedly flipping a fair coin, one will encounter a run of 5 heads in a row before one encounters a run of 2 tails in a row.
    \begin{solution}
        
    \end{solution}

    \item In a game similar to three card monte, the dealer places three cards on the table: the queen of spades and two red cards. The cards are placed in a row, and the queen starts in the center; the card configuration is thus $RQR$. The dealer proceeds to move. With each move, the dealer randomly switches the center card with one of the two edge cards (so the configuration after the first move is either $RRQ$ or $QRR$). What is the probability that, after 2025 moves, the center card is the queen?
\end{enumerate}























\end{document}