\documentclass[11pt]{article}
\usepackage{float}
% NOTE: Add in the relevant information to the commands below; or, if you'll be using the same information frequently, add these commands at the top of paolo-pset.tex file. 
\newcommand{\name}{Agust√≠n Esteva}
\newcommand{\email}{aesteva@uchicago.edu}
\newcommand{\classnum}{23500}
\newcommand{\subject}{Markov Chains, Martingales, and Brownian Motion}
\newcommand{\instructors}{Stephen Yearwood}
\newcommand{\assignment}{Problem Set 1}
\newcommand{\semester}{Spring 2025}
\newcommand{\duedate}{04/04/2025}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\Vol}{\text{Vol}}

%% blackboard bold math capitals
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}

%% script math capitals
\newcommand{\sA}{\mathscr{A}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sG}{\mathscr{G}}
\newcommand{\sH}{\mathscr{H}}
\newcommand{\sI}{\mathscr{I}}
\newcommand{\sJ}{\mathscr{J}}
\newcommand{\sK}{\mathscr{K}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sN}{\mathscr{N}}
\newcommand{\sO}{\mathscr{O}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\sS}{\mathscr{S}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sU}{\mathscr{U}}
\newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}}
\newcommand{\sX}{\mathscr{X}}
\newcommand{\sY}{\mathscr{Y}}
\newcommand{\sZ}{\mathscr{Z}}


\renewcommand{\emptyset}{\O}

\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\sm}{\setminus}


\newcommand{\sarr}{\rightarrow}
\newcommand{\arr}{\longrightarrow}

% NOTE: Defining collaborators is optional; to not list collaborators, comment out the line below.
%\newcommand{\collaborators}{Alyssa P. Hacker (\texttt{aphacker}), Ben Bitdiddle (\texttt{bitdiddle})}

\input{paolo-pset.tex}

% NOTE: To compile a version of this pset without problems, solutions, or reflections, uncomment the relevant line below.

%\excludeversion{problem}
%\excludeversion{solution}
%\excludeversion{reflection}

\begin{document}	
	
	% Use the \psetheader command at the beginning of a pset. 
	\psetheader

\section*{Problem 1}
\begin{problem}
    A ball is in any one of $n$ boxes and is in the $i$th box with probability $P_i$. If the ball is in box $i$, a search of that box will uncover it with probability $\alpha_i$. Find the conditional probability that the ball is in box $j$, given that a search of box $i$ did not uncover it.
\end{problem}
\begin{solution}
    Let $I_i$ denote the event that the ball is in box $i,$ $S_i$ denote the event of searching box $i$ and finding the ball. Then 
    \[\bbP\{S_i \mid I_i\} = \alpha_i\]
    \[\bbP\{I_i\} = P_i.\] We use Bayes' rule and his law of total probability:
    \begin{align*}
        \bbP\{I_j | S_i^c\} &= \frac{\bbP\{I_j \cap S_i^c\}}{\bbP\{S_i^c\}}\\
        &= \frac{\bbP\{S_i^c | I_j\}\bbP\{I_j\}}{1 - \bbP\{S_i\}}\\
        &= \frac{(1-\bbP\{S_i | I_j\})\bbP\{I_j\}}{1 - \sum_{k=1}^n\bbP\{S_i | I_k\}\bbP\{I_k\}}\\
        &= \boxed{\begin{cases}
            \frac{(1 - \alpha_i) P_i}{1-\alpha_iP_i}, \qquad i=j\\
            \frac{P_j}{1 - \alpha_iP_i}, \qquad \quad i\neq j
        \end{cases}}
    \end{align*}
    The second to last equality comes from the fact that the probability of succesfully finding the ball in a box where the ball is not is obviously $0.$
\end{solution}

\newpage
\section*{Problem 2}
\begin{problem}
    Given some fixed integer \( n \geq 1 \), consider the set of all possible permutations of the list \( (1, 2, \ldots, n) \). Suppose that one permutation \( (i_1, i_2, \ldots, i_n) \) is chosen at random from the set, with \( i_k \in \{1, 2, \ldots, n\} \) for every \( k \). Assuming that all permutations are equally likely to occur, i.e., each permutation is chosen with probability \( \frac{1}{n!} \), compute the probability \( p_m \) that exactly \( m \) of the numbers from the chosen permutation appear in their own positions (i.e., in the same positions in which they appear in the list \( (1, 2, \ldots, n) \)). What is the limit of these probabilities as \( n \) goes to infinity (\( m \) fixed)?
\end{problem}
\begin{solution}
The total number of possibilities is $n!.$ To calculate the favorable outcomes, we first need to fix $m$ items $\{1,2,\dots, n\}.$ There are $\binom{n}{m}$ ways of doing this. Once the $m$ numbers are fixed, we need to calculate the permutations of the $n-m$ elements of a set in which no element appears in its original position. This is called a derangement and is equal to 
\[!(n-m) = (n-m)! \sum_{k=0}^{n-m} \frac{(-1)^k}{k!}\]

Thus, if we call $E_{m,n}$ the event that $m$ numbers from a list of length $n$ are in their exact positions, 
\[\boxed{\bbP\{E_{m,n}\} = \frac{\binom{n}{m}\cdot !(n-m)}{n!} = \frac{\frac{n!}{(n-m)! m!} \cdot (n-m)! \sum_{k=0}^{n-m} \frac{(-1)^k}{k!}}{n!} = \frac{\sum_{k=0}^{n-m} \frac{(-1)^k}{k!}    }{m!}}\]
Using the Taylor series expansion of 
\[e^x = \sum_{k=0}^\infty \frac{x^k}{k!},\] we see that 
\[\boxed{\lim_{n\to \infty}\bbP\{E_{n,m}\} = \frac{1}{m!}e^{-1}}\]

\end{solution}

\newpage
\section*{Problem 3}

\begin{problem}
    
\end{problem}


\newpage
\section*{Problem 5}
\begin{problem}
    A fair coin is tossed repeatedly with results \( Y_0, Y_1, Y_2, \ldots \), where each \( Y_i \) is either \( 0 \) or \( 1 \) with probability \( \frac{1}{2} \). For \( n \geq 1 \), let 
\[
X_n = Y_n + Y_{n-1}
\]
be the number of \( 1 \)'s in the \( (n-1)^{\text{th}} \) and \( n^{\text{th}} \) tosses. Is \( \{X_n\}_{n \in \mathbb{N}} \) a Markov chain?
\end{problem}
\begin{solution}
    No. Consider 
    \[\bbP\{X_3 = 2 \mid X_1 = 1, X_2 = 1\} = 0\]
    \[\bbP\{X_3 = 2 \mid X_2 = 1\} = \frac{\bbP\{X_2 = 1 | X_3 = 2\}\bbP\{X_2 = 1\}}{\bbP\{X_2 = 1\}} = \frac{1}{2},\] since if $X_3 = 2,$ then necessarily, $Y_3 = 1$ and $Y_2 = 1,$ and thus either $X_2 = 1$ or $X_2 = 2.$ Thus, $\{X_n\}$ is \textbf{not} a Markov chain.
\end{solution}

\newpage
\section*{Problem 6}
\begin{problem}
    Five white balls and five black balls are distributed in two urns in such a way that each urn contains five balls. At each step, we draw one ball from each urn and exchange them. Let \( X_n \) be the number of white balls in the left urn at time \( n \).

\begin{enumerate}
    \item[(a)] Explain why \( \{X_n\} \) is a (time-homogeneous) Markov chain.
    \begin{solution}
    Define 
    \[L_n := \begin{cases}
        -1, \quad \text{white is picked from left urn on $n$th time}\\
        0 \quad \quad \;\text{else}
    \end{cases}\]  \[R_n := \begin{cases}
        1, \quad \;\;\;\text{white is picked from right urn on $n$th time}\\
        0 \quad \quad \;\text{else}
    \end{cases}\]
    Clearly, $L_{n+1}$ depends only on $X_n,$ since it is independent of the number of urns at any time before the present. Similarly, $R_{n + 1}$ is independent of any $X_i$ with $i < n.$ Since  $X_{n + 1} = X_n + (L_{n+1} + R_{n+1}),$ and $(L_{n + 1} + R_{n+1})$ are independent of any $X_i$ with $i<n,$ then for any $j \in \{0,1,2,3,4,5\},$ for any $s_0, \dots, s_n \in \{0,1,2,3,4,5\}$
    \begin{align*}
    \bbP\{X_{n + 1} &= j\mid X_n = s_n, X_{n-1} = s_{n-1}, \dots, X_0 = s_0\} = \bbP\{X_n + R_{n + 1} + L_{n + 1} = j \mid X_n = s_n, X_{n-1} = s_{n-1}, \dots, X_0 = s_0\}\\ &= \bbP\{X_n + R_{n + 1} + L_{n + 1} \mid X_n = s_n\}    
    \end{align*}

    The process is time independent if and only if, for any $x,y \in S = \{0,1,2,3,4,5\},$ we have $\bbP\{X_{n + 1} = x\mid X_n = y\} = \bbP\{X_1 = x \mid X_0 = y\}.$ From before, 
    \begin{align*}
        \bbP\{X_{n + 1} = x \mid X_n = y\} &= \bbP\{X_n + L_{n + 1} + R_{n + 1} =x\mid X_n = y\}\\
        &= \bbP\{L_{n + 1} + R_{n + 1}=x-y \mid X_n = y\}\\
        &= \bbP\{L_1 + R_1 = x-y \mid X_0 = y\}\\
        &= \bbP\{X_0 + L_1 + R_1  = x\mid X_0 = y\}\\
        &= \bbP\{X_1 = x \mid X_0 = y \}
    \end{align*}
    The key assumption being that $L_{n+1}$ and $R_{n+1}$ are independent from time $n,$ which is obvious, as they depend only on the number of balls at time $n.$
    
    \end{solution}
    \item[(b)] Find the state space and transition matrix for this Markov chain.
    \begin{solution}
        Clearly, \[\boxed{S = \{0,1,2,3,4,5\}},\] since at any point, there could be anywhere from $0$ to $5$ white balls in the left urn. 

        \[P = \begin{pmatrix}
            p(0,0) &p(0,1) & p(0,2) & p(0,3) & p(0,4) & p(0,5)\\
            p(1,0) & p(1,1) & p(1,2) & p(1,3) & p(1,4) & p(1,5)\\
            p(2,0) & p(2,1) & p(2,2) & p(2,3) & p(2,4) & p(2,5)\\
            p(3,0) & p(3,1) & p(3,2) & p(3,3) & p(3,4) & p(3,5)\\
            p(4,0) & p(4,1) & p(4,2) & p(4,3) & p(4,4) & p(4,5)\\
            p(5,0) & p(5,1) & p(5,2) & p(5,3) & p(5,4) & p(5,5)
        \end{pmatrix} = 
            \boxed{\begin{pmatrix}
            0 &1 & 0 & 0 & 0 & 0\\
            \frac{1}{25} & \frac{8}{25}& \frac{16}{25} & 0 & 0 & 0\\
            0 & \frac{4}{25}  & \frac{12}{25} & \frac{9}{25} & 0 & 0\\
            0 & 0 & \frac{9}{25} & \frac{12}{25} & \frac{4}{25} &0\\
            0 & 0 & 0 & \frac{16}{25} & \frac{8}{25} & \frac{1}{25}\\
            0 & 0 & 0 & 0 & 1 & 0
        \end{pmatrix}}\]
    \end{solution}
\end{enumerate}

\end{problem}

\newpage
\section*{Problem 7}
\begin{problem}
    An ant is sitting at vertex \( A \) of a regular tetrahedron. At the start of each minute, the ant (uniformly) randomly chooses one of the edges at the vertex it is currently sitting and crawls along that edge to an adjacent vertex. It takes one minute for the ant to crawl to the next vertex, at which point it chooses another edge (at random) and starts crawling again.

\begin{enumerate}
    \item[(a)] What is the probability that, after 6 minutes, the ant is back at vertex \( A \)?
    \begin{solution}
        Let the state space be 
        \[S  = \{A, B, C, D\}\]
        Let $X_n$ represent the vertex the ant is at on the $n-$th minute. Evidently, $\{X_n\}$ is a time-homogenous Markov chain with transition matrix:
        \[P = \begin{pmatrix}
            p(A,A) & p(A, B) & p(A, C) & p(A, D)\\
            p(B,A) & p(B, B) & p(B, C) & p(B, D)\\
            p(D,A) & p(C, B) & p(C, C) & p(C, D)\\
            p(D,A) & p(D, B) & p(D, C) & p(D, D)
        \end{pmatrix} = \begin{pmatrix}
            0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & 0 & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & 0 & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0
        \end{pmatrix}\] Thus, on the $6$th minute, the transition probability will be 
        \[P^6  = \begin{pmatrix}
\frac{61}{243} & \frac{182}{729} & \frac{182}{729} & \frac{182}{729} \\
\frac{182}{729} & \frac{61}{243} & \frac{182}{729} & \frac{182}{729} \\
\frac{182}{729} & \frac{182}{729} & \frac{61}{243} & \frac{182}{729} \\
\frac{182}{729} & \frac{182}{729} & \frac{182}{729} & \frac{61}{243}
\end{pmatrix}
\] Thus, we have that \[\boxed{p^6(A, A) = \frac{61}{243}}\] is the probability that starting at $A$ the ant will be back on $A$ after $6$ moves.
    \end{solution}
    \item[(b)] Do the same problem as in part (a), but this time the ant is crawling along the edges of a cube.
\begin{solution}
    Same scenario as before, but now 
    \[S = \{A, B, C, D, E, F, G, H\}\] then 
    \begin{align*}
        P &= \begin{pmatrix}
p(A, A) & p(A, B) & p(A, C) & p(A, D) & p(A, E) & p(A, F) & p(A, G) & p(A, H)\\
p(B, A) & p(B, B) & p(B, C) & p(B, D) & p(B, E) & p(B, F) & p(B, G) & p(B, H)\\
p(C, A) & p(C, B) & p(C, C) & p(C, D) & p(C, E) & p(C, F) & p(C, G) & p(C, H)\\
p(D, A) & p(D, B) & p(D, C) & p(D, D) & p(D, E) & p(D, F) & p(D, G) & p(D, H)\\
p(E, A) & p(E, B) & p(E, C) & p(E, D) & p(E, E) & p(E, F) & p(E, G) & p(E, H)\\
p(F, A) & p(F, B) & p(F, C) & p(F, D) & p(F, E) & p(F, F) & p(F, G) & p(F, H)\\
p(G, A) & p(G, B) & p(G, C) & p(G, D) & p(G, E) & p(G, F) & p(G, G) & p(G, H)\\
p(H, A) & p(H, B) & p(H, C) & p(H, D) & p(H, E) & p(H, F) & p(H, G) & p(H, H)
\end{pmatrix}\\
&= \begin{pmatrix}
0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0\\
\frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & 0 & 0\\
\frac{1}{3} & p(C, B) & p(C, C) & p(C, D) & p(C, E) & \frac{1}{3} & \frac{1}{3} & p(C, H)\\
\frac{1}{3} & p(D, B) & p(D, C) & p(D, D) & \frac{1}{3} & p(D, F) & \frac{1}{3} & p(D, H)\\
p(E, A) & \frac{1}{3} & p(E, C) & \frac{1}{3} & p(E, E) & p(E, F) & p(E, G) & \frac{1}{3}\\
p(F, A) & \frac{1}{3} & \frac{1}{3} & p(F, D) & p(F, E) & p(F, F) & p(F, G) & \frac{1}{3}\\
p(G, A) & p(G, B) & \frac{1}{3} & \frac{1}{3} & p(G, E) & p(G, F) & p(G, G) & \frac{1}{3}\\
p(H, A) & p(H, B) & p(H, C) & p(H, D) & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & p(H, H)
\end{pmatrix}\\
&= \begin{pmatrix}
0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0 \\
\frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & 0 & 0 \\
\frac{1}{3} & 0 & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & 0 \\
\frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 \\
0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} \\
0 & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0 & \frac{1}{3} \\
0 & 0 & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} \\
0 & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0
\end{pmatrix}
    \end{align*}
We use Wolfram and see that 
\[P^6 = \begin{pmatrix}
\frac{61}{243} & 0 & 0 & 0 & \frac{182}{729} & \frac{182}{729} & \frac{182}{729} & 0 \\
0 & \frac{61}{243} & \frac{182}{729} & \frac{182}{729} & 0 & 0 & 0 & \frac{182}{729} \\
0 & \frac{182}{729} & \frac{61}{243} & \frac{182}{729} & 0 & 0 & 0 & \frac{182}{729} \\
0 & \frac{182}{729} & \frac{182}{729} & \frac{61}{243} & 0 & 0 & 0 & \frac{182}{729} \\
\frac{182}{729} & 0 & 0 & 0 & \frac{61}{243} & \frac{182}{729} & \frac{182}{729} & 0 \\
\frac{182}{729} & 0 & 0 & 0 & \frac{182}{729} & \frac{61}{243} & \frac{182}{729} & 0 \\
\frac{182}{729} & 0 & 0 & 0 & \frac{182}{729} & \frac{182}{729} & \frac{61}{243} & 0 \\
0 & \frac{182}{729} & \frac{182}{729} & \frac{182}{729} & 0 & 0 & 0 & \frac{61}{243}
\end{pmatrix},\] and thus \[\boxed{p^6(A,A) = \frac{61}{243}}\]
\end{solution}
\end{enumerate}
\end{problem}
\newpage

\section*{Problem 8}
\begin{problem}
    Suppose \( \{X_n\} \) is a Markov chain with state space \( S = \{A, B, C, D\} \) and transition matrix
\[
\begin{pmatrix}
0 & \frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
0 & \frac{1}{2} & \frac{1}{2} & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & \frac{1}{2} & \frac{1}{2}
\end{pmatrix}.
\]

\begin{enumerate}
    \item[(a)] Draw a (directed) graph depicting the states and the corresponding transition probabilities (i.e., label each edge that you draw with the probability of traversing it). Explain why \( \{X_n\} \) is irreducible.
    \begin{solution}
Graph:
        \begin{center}
\begin{tikzpicture}[scale=1.2, every node/.style={draw, circle, inner sep=1pt}]
    \node (A) at (-2,0) {A};
    \node (B) at (0,2) {B};
    \node (C) at (2,0) {C};
    \node (D) at (0,-2) {D};

    \draw[->] (A) -- (B) node[midway, above left] {\textcolor{red}{$\frac{1}{4}$}};
    \draw[->, loop above] (B) edge node {\textcolor{red}{$\frac{1}{2}$}} (B);
    \draw[->] (B) -- (C) node[midway, above right] {\textcolor{red}{$\frac{1}{2}$}};
    \draw[->] (B) -- (C) node[midway, above right] {\textcolor{red}{$\frac{1}{2}$}};
    
    \draw[->] (A) -- (C) node[midway, below] {\textcolor{red}{$\frac{1}{2}$}};
    \draw[->, bend right=20] (C) to node[midway, above] {\textcolor{red}{$1$}} (A);
    \draw[->] (A) -- (D) node[midway, below left] {\textcolor{red}{$\frac{1}{4}$}};
    \draw[->, loop below] (D) edge node {\textcolor{red}{$\frac{1}{2}$}} (D);
    \draw[->] (D) -- (C) node[midway, below right] {\textcolor{red}{$\frac{1}{2}$}};
\end{tikzpicture}
\end{center}
$\{X_n\}$ is irreducible. For the following table, the first number is the smallest number of turns it takes for $p^n(i,j) >0.$ 
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|l|l|l|} \hline 
         &   A&B & C&D\\ \hline 
         A& 
     $3 \text{ w.p. } \geq\frac{1}{16}$ & $1 \text{ w.p. }\frac{1}{4}$& $2 \text{ w.p. }\geq\frac{1}{8}$ &$1 \text{ w.p. }\frac{1}{4}$\\ \hline 
 B&$2 \text{ w.p. }\geq\frac{1}{2}$ &$1 \text{ w.p. }\frac{1}{2}$ & $1 \text{ w.p. }\frac{1}{2}$&$3\text{ w.p. }\geq\frac{1}{8}$\\ \hline 
 C&$1 \text{ w.p. }{1}$ &$2 \text{ w.p. }\geq\frac{1}{4}$ &$3 \text{ w.p. }\geq\frac{1}{8}$ &$2 \text{ w.p. }\geq\frac{1}{4}$\\ \hline 
 D&$2 \text{ w.p. }\geq \frac{1}{4}$ &$2 \text{ w.p. }\geq\frac{1}{8}$ &$1 \text{ w.p. }\frac{1}{2}$ &$1 \text{ w.p. }\frac{1}{2}$\\ \hline\end{tabular}
    \caption{Irreducibility} 
\end{table}
Thus, for every $i,j \in S,$ there exists some $n,m>0$ such that $p^n(i,j) >0$ and $p^m(j,i) >0,$ as you can see in the table above. 
    \end{solution}
    \item[(b)] Assume that the chain starts with \( X_0 = B \). What is the probability that \( X_3 = C \)?
\begin{solution}
    Using Wolfram, we see that 
    \[P^3 = 
\begin{pmatrix}
\frac{1}{4} & \frac{3}{16} & \frac{3}{8} & \frac{3}{16} \\
\frac{1}{4} & \frac{1}{4} & \frac{3}{8} & \frac{1}{8} \\
\frac{1}{2} & \frac{1}{8} & \frac{1}{4} & \frac{1}{8} \\
\frac{1}{4} & \frac{1}{8} & \frac{3}{8} & \frac{1}{4}
\end{pmatrix}
\] and thus 
\[\boxed{p^3(B, C) = \frac{3}{8}}\]

\end{solution}
    \item[(c)] Under the same assumption as in the previous part, what is the probability that \( X_3 = C \), \( X_4 = A \), and \( X_6 = B \) all occur?
\begin{solution}
We use the Markov property and the transition matrices above:
\begin{align*}
 \bbP\{X_3 = C, X_4 = A, X_6 = B \mid X_0 = B\} &= \bbP\{X_4 = A, X_6= B\mid X_3 = C\}\bbP\{X_3 = C \mid X_0 = B\}   \\
 &= \bbP\{X_6 = B \mid X_4 = A\}\bbP\{X_4 = A \mid X_3 = C\}\bbP\{X_3 = C \mid X_0 = B\}\\
 &= p^3(B,C)p(C,A)p^2(A.B)\\
 &= \frac{3}{64}
\end{align*}
We compute the first term above, $P(C,A) =1$ is given by $P$ and the last term is given by $P^2$ (not shown).
\end{solution}
    \item[(d)] Compute the invariant probability vector for this Markov chain.
\begin{solution}
    We need to find some $\pi = (\pi_1, \pi_2, \pi_3, \pi_4)$ probability vector such that 
    \[\pi P  = \pi \iff P^T\pi^T = \pi^T \iff (P^T - I)\pi^T = 0\iff (4P^T - 4I)\pi^T = 0\] 
    We know that 
    \[P = \begin{pmatrix}
0 & \frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
0 & \frac{1}{2} & \frac{1}{2} & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & \frac{1}{2} & \frac{1}{2}
\end{pmatrix} \implies 4P^T = \begin{pmatrix}
    0 &0 &4 &0\\
    1 & 2 & 0 &0 \\
    2 & 2 & 0 & 2\\
    1 & 0 & 0 & 2
\end{pmatrix}\]
    
    Thus, we need to find $N(4P^T - 4I):$
    \begin{align*}
        N(4P^T - 4I) &= 
        \begin{pmatrix}
    -4 &0 &4 &0\\
    1 & -2 & 0 &0 \\
    2 & 2 & -4 & 2\\
    1 & 0 & 0 & -2
\end{pmatrix}\\
        &\simeq
        \begin{pmatrix}
    -1 &0 &1 &0\\
    1 & -2 & 0 &0 \\
    1 & 1 & -2 & 1\\
    1 & 0 & 0 & -2
\end{pmatrix}\\
&\simeq
        \begin{pmatrix}
    -1 &0 &1 &0\\
    0 & -2 & 1 &0 \\
    0 & 1 & -1 & 1\\
    0 & 0 & 1 & -2
\end{pmatrix}\\
&\simeq
        \begin{pmatrix}
    1 &0 &0 &-2\\
    0 & 1 & 0 & -1\\
    0 & 0 & 1 &-2 \\
    0 & 0 & 0 & 0
\end{pmatrix}\\
    \end{align*}
    Call $\pi^T = x$ for simplification. We get the following four equations:
    \begin{align}
        x_1 = 2x_4
    \end{align}
    \begin{align}
        x_2 = x_4
    \end{align}
    \begin{align}
        x_3 = 2x_4
    \end{align}
    \begin{align}
        x_1 + x_2 + x_3 + x_4 = 1
    \end{align}
Solving:
\[x = \begin{pmatrix}
    \frac{1}{3}\\
    \frac{1}{6}\\
    \frac{1}{3}\\
    \frac{1}{6}
\end{pmatrix}\] and thus \[\boxed{\pi = (\frac{1}{3},
    \frac{1}{6},
    \frac{1}{3},
    \frac{1}{6})}\] is our invariant probability vector. 

\end{solution}
    \item[(e)] Let \( J_n \) denote the probability that at time \( n \) the system has been in state \( C \) exactly the same number of times it has been in state \( A \). Find
    \[
    \lim_{n \to \infty} J_n.
    \] Here again suppose that $X_0 = B$
\end{enumerate}
\begin{solution}


Define 
\[N_C = |\{X_k = C \mid k\leq N\}|\]
\[N_A = |\{X_k = A\mid k\leq N\}|\] Note that 
\[N_A = \begin{cases}
    1 + N_C, \qquad X_N = C\\
    N_C, \quad \;\;\qquad X_N \neq C
\end{cases}\]
Thus, 
\[J_N = \{N_C = N_A\} = \{X_N \neq C\}.\] Thus, 
\[\lim_{N\to \infty}\bbP\{J_N\} = \lim_{N\to \infty}\bbP\{X_N \neq C\}= 1-\pi_C = 1- \frac{1}{3} = \boxed{\frac{2}{3}}\]
\end{solution}
\end{problem}

\newpage
\section*{Problem 9}
\begin{problem}
    Let \( G \) be a finite, connected graph (i.e., any two vertices can be joined by a path of edges). Let \( X_n \) and \( Y_n \) be independent lazy random walks on \( G \), meaning that at each step \( X_n \) stays put with probability \( \frac{1}{2} \) and moves to a uniformly chosen neighboring vertex with probability \( \frac{1}{2} \), and similarly for \( Y_n \). In other words, the transition probabilities for \(\{X_n\}\) (and also for \(\{Y_n\}\)) are given by

\[
p(x, y) = \begin{cases} 
\frac{1}{2}, & \text{if } x = y \\
\frac{1}{2 \deg x}, & \text{if } x \neq y \text{ and } x \text{ and } y \text{ are joined by an edge} \\
0, & \text{otherwise}.
\end{cases}
\]



(a) Show that for any possible starting points \( X_0  = x_0\) and \( Y_0= y_0 \), with probability 1, there exists \( n \) such that \( X_n = Y_n \).
\begin{solution}
Enumerate the vertices of $G(V,E)$ by $V(G) = \{1,2,\dots, N\}.$
Consider the process $Z_n =(X_n, Y_n)$ taking place in the state space $V(G)\times V(G).$ We claim that $\{Z_n\}$ is a Markov process. First, notice that 
\[p_Z(v,v') = p_Z((x,y), (x', y')) = \bbP\{X_1 = x', Y_1' = y' \mid X_0 = x, Y_0= y  = \begin{cases}
    p(x,x')p(y,y'). \quad x\neq y\\
    p(x,x'), \qquad \quad \;\:\;\: x = y
\end{cases}\}\] Thus, $\{Z_n\}$ acts like $\{X_n\}$ once the chains couple up. We claim that if $\tau = \min\{n \geq 0\mid X_n = Y_n\} = \min\{n \geq 0 \mid Z_n = (z,z). z\in V(G)\},$ then
\[\bbP\{\tau < \infty \mid Z_0 = (x_0, y_0)\} = 1.\] To do this, it suffices to show that $Z_n$ is an irreducible and recurrent Markov chain, since they hit all the states in $S$ with probability $1.$ 

(Markov Chain) Let $v_0, v_1, \dots, v_n \in V(G) \times V(G).$ then by definition,
\begin{align*}
\bbP\{Z_n = v_n \mid Z_0 = v_0, \dots, Z_{n-1 } = v_{n-1}\} &= \bbP\{(X_n, Y_n) = (x_n, y_n) \mid (X_0, Y_0) = (x_0, y_0), \dots, (X_{n-1  }, Y_{n-1}) = (x_{n-1}, y_{n-1})\}\\
&= \bbP\{X_n = x_n \mid X_0 = x_0, \dots, X_{n-1} = x_{n-1}\} \bbP\{Y_n = u_n \mid Y_0 = y_0, \dots, Y_{n-1} = y_{n-1}\}\\
&= \bbP\{X_n = x_n \mid X_{n-1} = x_{n-1}\} \bbP\{Y_n = u_n \mid Y_{n-1} = y_{n-1}\}\\
&= \bbP\{(X_n, Y_n) = (x_n, y_n) \mid (X_{n-1   }, Y_{n-1}) = (x_{n-1}, y_{n-1})\}\\
&= \bbP\{Z_n = v_n \mid Z_{n-1} = v_{n-1}\}
\end{align*}

(Irreducible) It suffices to show that for any $v, w \in S \times S,$ there exists some $n >0$ such that $p^n(v,w) >0.$ But if $v =(x,y)$ and $w = (x', y'),$ then since $X_n$ is irreducible (since the graph is connected), there exists some $n_x>0$ such that $p^{n_x}(x,x')>0$ and similarly $p^{n_y}(y,y')>0.$ Let $n = \max\{n_x, n_y\},$ then 
\[p^n((x, y), (x', y')) = p^n(x,x') p^n(y,y') \geq p^{n_x}(x,x') p^{n_y}(y,y') >0.\] Since $S \times S$ is irreducible, then it is recurrent. Note the importance that $X_n$ and $Y_n$ are lazy, since this guarantees that 
\[p^n(v,v) >0.\]
Thus, by a theorem done in class, since $S\times S$ is finite and $\{Z_n\}$ is recurrent and $(x,x) \in S \times S,$ then with probability one, $\bbP\{Z_n = (x,x)\}.$ Thus, $X_n = x$ and $Y_n = x$ for some $n$ with probability one.




Here is another proof which works in the degenerate case when $G(V,E)$ is a circle.
 Consider the process $\{Z_n\},$ which is defined as
\[Z_n := \text{min \# edges between $X_n$ and $Y_n$}\]
Since $G(V,E)$ is finite and connected, the state space of $Z_n$ is $S_Z = \{0, 1, \dots, K\},$ where $K\leq \frac{N(N-1)}{2}.$ We claim that $\{Z_n\}$ is a Markov chain with a single communication class that is recurrent and aperiodic. 

To see that $\{Z_n\}$ is a Markov chain, note that $Z_{n+1}$ depends only on $X_{n}$ and $Y_{n}.$ By independence, $\{X_n\}$ is independent of $Y_{n-1}.$  Similarly, $Y_{n}$ is independent of $X_{n-1}.$ Since both are Markov chains, then $X_n$ is independent of $X_{n-1}$ and $Y_n$ is independent of $Y_{n-1}.$ Thus, since $X_n$ and $Y_n$ completely determine $Z_n,$ then $Z_{n + 1}$ depends only on $Z_n$ and is independent of $Z_{n-1}.$ 

TO see that $\{Z_n\}$ has one communication class, we suppose otherwise. That is, for some $i,j \in S,$ and for all $n>0,$ $p^n(i,j)  =  0.$ Without loss of generality, suppose $i<j$ and let $k = j-i$ We know that 
$p^n(i,j) \geq p^{n-1}(i,{j-1})p(i,j-1) \geq \dots \geq p^{n-k}(i,i)p(i,i + 1)\cdots p(i,j-1).$ It suffices to show that each terms is positive. We know that, for any $i,$
$p(i, i + 1) >0$ since there is a chance $X_n$ stays at some $x \in S$ and $Y_n$ leaves, resulting in an edge of $1.$ Similarly, $p^{n-k}(i,i) \geq \frac{1}{2^{n-k +1}}.$ Thus, there exists some $n \in \bbN$ such that $p^n(i,j) >0$ for all $i,j.$ This also shows that $\{Z_n\}$ is recurrent.

$\{Z_n\}$ is aperiodic since $p(i,i) >0.$ 

By a theorem in class, we have that for any $s \in S$
\[\lim_{n\to \infty }\bbP\{Z_n = 0 \text{ i.o. }\mid s\} = 1\] Thus, with probability $1,$ there exists some $M>0$ large such that $Z_n = 0$ and thus $X_M = Y_M.$
\end{solution}

(b) Give an example of a graph \( G \) for which the conclusion of part (a) fails if \( X_n \) and \( Y_n \) are ordinary random walks rather than lazy random walks (i.e., \( X_{n+1} \) is never equal to \( X_n \) and \( Y_{n+1} \) is never equal to \( Y_n \).
\begin{solution}
Consider the following graph $G:$
    \begin{center}
\begin{tikzpicture}[scale=1.2, every node/.style={draw, circle, inner sep=1pt}]
    \node (A) at (-2,0) {1};
    \node (B) at (2,0) {2};

    \draw[-] (A) -- (B) node[midway, above left] {\textcolor{red}{${1}$}};
    \end{tikzpicture}
\end{center}
And consider when $X_0 = 1$ and $Y_0 = y.$ Then since $X_n = S\setminus\{Y_n\},$ the Markov chains will never be the same.
\end{solution}

\end{problem}

\newpage
\section*{Problem 10}
\begin{problem}
    Let $P$ be irreducible and aperiodic, 
\end{problem}

\end{document}