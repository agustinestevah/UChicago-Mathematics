\documentclass[11pt]{article}
\usepackage{float}
% NOTE: Add in the relevant information to the commands below; or, if you'll be using the same information frequently, add these commands at the top of paolo-pset.tex file. 
\newcommand{\name}{Agust√≠n Esteva}
\newcommand{\email}{aesteva@uchicago.edu}
\newcommand{\classnum}{23500}
\newcommand{\subject}{Markov Chains, Martingales, and Brownian Motion}
\newcommand{\instructors}{Stephen Yearwood}
\newcommand{\assignment}{Problem Set 1}
\newcommand{\semester}{Spring 2025}
\newcommand{\duedate}{04/04/2025}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\Vol}{\text{Vol}}

%% blackboard bold math capitals
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}

%% script math capitals
\newcommand{\sA}{\mathscr{A}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sG}{\mathscr{G}}
\newcommand{\sH}{\mathscr{H}}
\newcommand{\sI}{\mathscr{I}}
\newcommand{\sJ}{\mathscr{J}}
\newcommand{\sK}{\mathscr{K}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sN}{\mathscr{N}}
\newcommand{\sO}{\mathscr{O}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\sS}{\mathscr{S}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sU}{\mathscr{U}}
\newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}}
\newcommand{\sX}{\mathscr{X}}
\newcommand{\sY}{\mathscr{Y}}
\newcommand{\sZ}{\mathscr{Z}}


\renewcommand{\emptyset}{\O}

\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\sm}{\setminus}


\newcommand{\sarr}{\rightarrow}
\newcommand{\arr}{\longrightarrow}

% NOTE: Defining collaborators is optional; to not list collaborators, comment out the line below.
%\newcommand{\collaborators}{Alyssa P. Hacker (\texttt{aphacker}), Ben Bitdiddle (\texttt{bitdiddle})}

\input{paolo-pset.tex}

% NOTE: To compile a version of this pset without problems, solutions, or reflections, uncomment the relevant line below.

%\excludeversion{problem}
%\excludeversion{solution}
%\excludeversion{reflection}

\begin{document}	
	
	% Use the \psetheader command at the beginning of a pset. 
	\psetheader

\section*{Problem 1}
\begin{problem}
    A ball is in any one of $n$ boxes and is in the $i$th box with probability $P_i$. If the ball is in box $i$, a search of that box will uncover it with probability $\alpha_i$. Find the conditional probability that the ball is in box $j$, given that a search of box $i$ did not uncover it.
\end{problem}
\begin{solution}
    Let $I_i$ denote the event that the ball is in box $i,$ $S_i$ denote the event of searching box $i$ and finding the ball. Then 
    \[\bbP\{S_i \mid I_i\} = \alpha_i\]
    \[\bbP\{I_i\} = P_i.\] We use Bayes' rule and his law of total probability:
    \begin{align*}
        \bbP\{I_j | S_i^c\} &= \frac{\bbP\{I_j \cap S_i^c\}}{\bbP\{S_i^c\}}\\
        &= \frac{\bbP\{S_i^c | I_j\}\bbP\{I_j\}}{1 - \bbP\{S_i\}}\\
        &= \frac{(1-\bbP\{S_i | I_j\})\bbP\{I_j\}}{1 - \sum_{k=1}^n\bbP\{S_i | I_k\}\bbP\{I_k\}}\\
        &= \frac{1 \cdot P_j}{1 - \alpha_iP_i}\\
        &= \frac{P_j}{1 - \alpha_iP_i}
    \end{align*}
    The second to last equality comes from the fact that the probability of succesfully finding the ball in a box where the ball is not is obviously $0.$
\end{solution}

\newpage
\section*{Problem 2}
\begin{problem}
    Given some fixed integer \( n \geq 1 \), consider the set of all possible permutations of the list \( (1, 2, \ldots, n) \). Suppose that one permutation \( (i_1, i_2, \ldots, i_n) \) is chosen at random from the set, with \( i_k \in \{1, 2, \ldots, n\} \) for every \( k \). Assuming that all permutations are equally likely to occur, i.e., each permutation is chosen with probability \( \frac{1}{n!} \), compute the probability \( p_m \) that exactly \( m \) of the numbers from the chosen permutation appear in their own positions (i.e., in the same positions in which they appear in the list \( (1, 2, \ldots, n) \)). What is the limit of these probabilities as \( n \) goes to infinity (\( m \) fixed)?
\end{problem}
\begin{solution}
    Fix $n.$ We induct on $m.$ 
    \begin{enumerate}
        \item Suppose that $m = 1,$ then 
        \[\]
    \end{enumerate}
\end{solution}


\newpage
\section*{Problem 5}
\begin{problem}
    A fair coin is tossed repeatedly with results \( Y_0, Y_1, Y_2, \ldots \), where each \( Y_i \) is either \( 0 \) or \( 1 \) with probability \( \frac{1}{2} \). For \( n \geq 1 \), let 
\[
X_n = Y_n + Y_{n-1}
\]
be the number of \( 1 \)'s in the \( (n-1)^{\text{th}} \) and \( n^{\text{th}} \) tosses. Is \( \{X_n\}_{n \in \mathbb{N}} \) a Markov chain?
\end{problem}
\begin{solution}
    No. Consider 
    \[\bbP\{X_3 = 2 \mid X_1 = 1, X_2 = 1\} = 0\]
    \[\bbP\{X_3 = 2 \mid X_2 = 1\} = \frac{\bbP\{X_2 = 1 | X_3 = 2\}\bbP\{X_2 = 1\}}{\bbP\{X_2 = 1\}} = \frac{1}{2},\] since if $X_3 = 2,$ then necessarily, $Y_3 = 1$ and $Y_2 = 1,$ and thus either $X_2 = 1$ or $X_2 = 2.$ Thus, $\{X_n\}$ is not a Markov chain.
\end{solution}

\newpage
\section*{Problem 6}
\begin{problem}
    Five white balls and five black balls are distributed in two urns in such a way that each urn contains five balls. At each step, we draw one ball from each urn and exchange them. Let \( X_n \) be the number of white balls in the left urn at time \( n \).

\begin{enumerate}
    \item[(a)] Explain why \( \{X_n\} \) is a (time-homogeneous) Markov chain.
    \begin{solution}
    Define 
    \[L_n := \begin{cases}
        -1, \quad \text{white is picked from left urn on $n$th time}\\
        0 \quad \quad \;\text{else}
    \end{cases}\]  \[R_n := \begin{cases}
        1, \quad \;\;\;\text{white is picked from right urn on $n$th time}\\
        0 \quad \quad \;\text{else}
    \end{cases}\]
    Clearly, $L_{n+1}$ depends only on $X_n,$ since it is independent of the number of urns at any time before the present. Similarly, $R_{n + 1}$ is independent of any $X_i$ with $i < n.$ Since  $X_{n + 1} = X_n + (L_{n+1} + R_{n+1}),$ and $(L_{n + 1} + R_{n+1})$ are independent of any $X_i$ with $i<n,$ then for any $j \in \{0,1,2,3,4,5\},$ for any $s_0, \dots, s_n \in \{0,1,2,3,4,5\}$
    \begin{align*}
    \bbP\{X_{n + 1} &= j\mid X_n = s_n, X_{n-1} = s_{n-1}, \dots, X_0 = s_0\} = \bbP\{X_n + R_{n + 1} + L_{n + 1} = j \mid X_n = s_n, X_{n-1} = s_{n-1}, \dots, X_0 = s_0\}\\ &= \bbP\{X_n + R_{n + 1} + L_{n + 1} \mid X_n = s_n\}    
    \end{align*}

    The process is time independent if and only if, for any $x,y \in S = \{0,1,2,3,4,5\},$ we have $\bbP\{X_{n + 1} = x\mid X_n = y\} = \bbP\{X_1 = x \mid X_0 = y\}.$ From before, 
    \begin{align*}
        \bbP\{X_{n + 1} = x \mid X_n = y\} &= \bbP\{X_n + L_{n + 1} + R_{n + 1} =x\mid X_n = y\}\\
        &= \bbP\{L_{n + 1} + R_{n + 1}=x-y \mid X_n = y\}\\
        &= \bbP\{L_1 + R_1 = x-y \mid X_0 = y\}\\
        &= \bbP\{X_0 + L_1 + R_1  = x\mid X_0 = y\}\\
        &= \bbP\{X_1 = x \mid X_0 = y \}
    \end{align*}
    The key assumption being that $L_{n+1}$ and $R_{n+1}$ are independent from time $n,$ which is obvious, as they depend only on the number of balls at time $n.$
    
    \end{solution}
    \item[(b)] Find the state space and transition matrix for this Markov chain.
    \begin{solution}
        Clearly, $S = \{0,1,2,3,4,5\},$ since at any point, there could be anywhere from $0$ to $5$ white balls in the left urn. 

        \[P = \begin{pmatrix}
            p(0,0) &p(0,1) & p(0,2) & p(0,3) & p(0,4) & p(0,5)\\
            p(1,0) & p(1,1) & p(1,2) & p(1,3) & p(1,4) & p(1,5)\\
            p(2,0) & p(2,1) & p(2,2) & p(2,3) & p(2,4) & p(2,5)\\
            p(3,0) & p(3,1) & p(3,2) & p(3,3) & p(3,4) & p(3,5)\\
            p(4,0) & p(4,1) & p(4,2) & p(4,3) & p(4,4) & p(4,5)\\
            p(5,0) & p(5,1) & p(5,2) & p(5,3) & p(5,4) & p(5,5)
        \end{pmatrix} = 
            \begin{pmatrix}
            0 &1 & 0 & 0 & 0 & 0\\
            \frac{1}{25} & \frac{8}{25}& \frac{16}{25} & 0 & 0 & 0\\
            0 & \frac{4}{25}  & \frac{12}{25} & \frac{9}{25} & 0 & 0\\
            0 & 0 & \frac{9}{25} & \frac{12}{25} & \frac{4}{25} &0\\
            0 & 0 & 0 & \frac{16}{25} & \frac{8}{25} & \frac{1}{25}\\
            0 & 0 & 0 & 0 & 1 & 0
        \end{pmatrix}\]
    \end{solution}
\end{enumerate}

\end{problem}

\newpage
\section*{Problem 7}
\begin{problem}
    An ant is sitting at vertex \( A \) of a regular tetrahedron. At the start of each minute, the ant (uniformly) randomly chooses one of the edges at the vertex it is currently sitting and crawls along that edge to an adjacent vertex. It takes one minute for the ant to crawl to the next vertex, at which point it chooses another edge (at random) and starts crawling again.

\begin{enumerate}
    \item[(a)] What is the probability that, after 6 minutes, the ant is back at vertex \( A \)?
    \begin{solution}
        Let the state space be 
        \[S  = \{A, B, C, D\}\]
        Let $X_n$ represent the vertex the ant is at on the $n-$th minute. Evidently, $\{X_n\}$ is a time-homogenous Markov chain with transition matrix:
        \[P = \begin{pmatrix}
            p(A,A) & p(A, B) & p(A, C) & p(A, D)\\
            p(B,A) & p(B, B) & p(B, C) & p(B, D)\\
            p(D,A) & p(C, B) & p(C, C) & p(C, D)\\
            p(D,A) & p(D, B) & p(D, C) & p(D, D)
        \end{pmatrix} = \begin{pmatrix}
            0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & 0 & \frac{1}{3} & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & 0 & \frac{1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0
        \end{pmatrix}\] Thus, on the $6$th minute, the transition probability will be 
        \[P^6  = \begin{pmatrix}
\frac{61}{243} & \frac{182}{729} & \frac{182}{729} & \frac{182}{729} \\
\frac{182}{729} & \frac{61}{243} & \frac{182}{729} & \frac{182}{729} \\
\frac{182}{729} & \frac{182}{729} & \frac{61}{243} & \frac{182}{729} \\
\frac{182}{729} & \frac{182}{729} & \frac{182}{729} & \frac{61}{243}
\end{pmatrix}
\] Thus, we have that \[\boxed{p^6(A, A) = \frac{61}{243}}\] is the probability that starting at $A$ the ant will be back on $A$ after $6$ moves.
    \end{solution}
    \item[(b)] Do the same problem as in part (a), but this time the ant is crawling along the edges of a cube.
\begin{solution}
    Same scenario as before, but now 
    \[S = \{A, B, C, D, E, F, G, H\}\] then 
    \begin{align*}
        P &= \begin{pmatrix}
p(A, A) & p(A, B) & p(A, C) & p(A, D) & p(A, E) & p(A, F) & p(A, G) & p(A, H)\\
p(B, A) & p(B, B) & p(B, C) & p(B, D) & p(B, E) & p(B, F) & p(B, G) & p(B, H)\\
p(C, A) & p(C, B) & p(C, C) & p(C, D) & p(C, E) & p(C, F) & p(C, G) & p(C, H)\\
p(D, A) & p(D, B) & p(D, C) & p(D, D) & p(D, E) & p(D, F) & p(D, G) & p(D, H)\\
p(E, A) & p(E, B) & p(E, C) & p(E, D) & p(E, E) & p(E, F) & p(E, G) & p(E, H)\\
p(F, A) & p(F, B) & p(F, C) & p(F, D) & p(F, E) & p(F, F) & p(F, G) & p(F, H)\\
p(G, A) & p(G, B) & p(G, C) & p(G, D) & p(G, E) & p(G, F) & p(G, G) & p(G, H)\\
p(H, A) & p(H, B) & p(H, C) & p(H, D) & p(H, E) & p(H, F) & p(H, G) & p(H, H)
\end{pmatrix}\\
&= \begin{pmatrix}
0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0\\
\frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & 0 & 0\\
\frac{1}{3} & p(C, B) & p(C, C) & p(C, D) & p(C, E) & \frac{1}{3} & \frac{1}{3} & p(C, H)\\
\frac{1}{3} & p(D, B) & p(D, C) & p(D, D) & \frac{1}{3} & p(D, F) & \frac{1}{3} & p(D, H)\\
p(E, A) & \frac{1}{3} & p(E, C) & \frac{1}{3} & p(E, E) & p(E, F) & p(E, G) & \frac{1}{3}\\
p(F, A) & \frac{1}{3} & \frac{1}{3} & p(F, D) & p(F, E) & p(F, F) & p(F, G) & \frac{1}{3}\\
p(G, A) & p(G, B) & \frac{1}{3} & \frac{1}{3} & p(G, E) & p(G, F) & p(G, G) & \frac{1}{3}\\
p(H, A) & p(H, B) & p(H, C) & p(H, D) & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & p(H, H)
\end{pmatrix}\\
&= \begin{pmatrix}
0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0 \\
\frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & 0 & 0 \\
\frac{1}{3} & 0 & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & 0 \\
\frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 \\
0 & \frac{1}{3} & 0 & \frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} \\
0 & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0 & \frac{1}{3} \\
0 & 0 & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} \\
0 & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0
\end{pmatrix}
    \end{align*}
We use Wolfram and see that 
\[P^6 = \begin{pmatrix}
\frac{61}{243} & 0 & 0 & 0 & \frac{182}{729} & \frac{182}{729} & \frac{182}{729} & 0 \\
0 & \frac{61}{243} & \frac{182}{729} & \frac{182}{729} & 0 & 0 & 0 & \frac{182}{729} \\
0 & \frac{182}{729} & \frac{61}{243} & \frac{182}{729} & 0 & 0 & 0 & \frac{182}{729} \\
0 & \frac{182}{729} & \frac{182}{729} & \frac{61}{243} & 0 & 0 & 0 & \frac{182}{729} \\
\frac{182}{729} & 0 & 0 & 0 & \frac{61}{243} & \frac{182}{729} & \frac{182}{729} & 0 \\
\frac{182}{729} & 0 & 0 & 0 & \frac{182}{729} & \frac{61}{243} & \frac{182}{729} & 0 \\
\frac{182}{729} & 0 & 0 & 0 & \frac{182}{729} & \frac{182}{729} & \frac{61}{243} & 0 \\
0 & \frac{182}{729} & \frac{182}{729} & \frac{182}{729} & 0 & 0 & 0 & \frac{61}{243}
\end{pmatrix},\] and thus \[\boxed{p^6(A,A) = \frac{61}{243}}\]
\end{solution}
\end{enumerate}
\end{problem}
\newpage

\section*{Problem 8}
\begin{problem}
    Suppose \( \{X_n\} \) is a Markov chain with state space \( S = \{A, B, C, D\} \) and transition matrix
\[
\begin{pmatrix}
0 & \frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
0 & \frac{1}{2} & \frac{1}{2} & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & \frac{1}{2} & \frac{1}{2}
\end{pmatrix}.
\]

\begin{enumerate}
    \item[(a)] Draw a (directed) graph depicting the states and the corresponding transition probabilities (i.e., label each edge that you draw with the probability of traversing it). Explain why \( \{X_n\} \) is irreducible.
    \begin{solution}
Graph:
        \begin{center}
\begin{tikzpicture}[scale=1.2, every node/.style={draw, circle, inner sep=1pt}]
    \node (A) at (-2,0) {A};
    \node (B) at (0,2) {B};
    \node (C) at (2,0) {C};
    \node (D) at (0,-2) {D};

    \draw[->] (A) -- (B) node[midway, above left] {\textcolor{red}{$\frac{1}{4}$}};
    \draw[->, loop above] (B) edge node {\textcolor{red}{$\frac{1}{2}$}} (B);
    \draw[->] (B) -- (C) node[midway, above right] {\textcolor{red}{$\frac{1}{2}$}};
    \draw[->] (B) -- (C) node[midway, above right] {\textcolor{red}{$\frac{1}{2}$}};
    
    \draw[->] (A) -- (C) node[midway, below] {\textcolor{red}{$\frac{1}{2}$}};
    \draw[->, bend right=20] (C) to node[midway, above] {\textcolor{red}{$1$}} (A);
    \draw[->] (A) -- (D) node[midway, below left] {\textcolor{red}{$\frac{1}{4}$}};
    \draw[->, loop below] (D) edge node {\textcolor{red}{$\frac{1}{2}$}} (D);
    \draw[->] (D) -- (C) node[midway, below right] {\textcolor{red}{$\frac{1}{2}$}};
\end{tikzpicture}
\end{center}
$\{X_n\}$ is irreducible. For the following table, the first number is the smallest number of turns it takes for $p^n(i,j) >0.$ 
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|l|l|l|} \hline 
         &   A&B & C&D\\ \hline 
         A& 
     $3 \text{ w.p. } \geq\frac{1}{16}$ & $1 \text{ w.p. }\frac{1}{4}$& $2 \text{ w.p. }\geq\frac{1}{8}$ &$1 \text{ w.p. }\frac{1}{4}$\\ \hline 
 B&$2 \text{ w.p. }\geq\frac{1}{2}$ &$1 \text{ w.p. }\frac{1}{2}$ & $1 \text{ w.p. }\frac{1}{2}$&$3\text{ w.p. }\geq\frac{1}{8}$\\ \hline 
 C&$1 \text{ w.p. }{1}$ &$2 \text{ w.p. }\geq\frac{1}{4}$ &$3 \text{ w.p. }\geq\frac{1}{8}$ &$2 \text{ w.p. }\geq\frac{1}{4}$\\ \hline 
 D&$2 \text{ w.p. }\geq \frac{1}{4}$ &$2 \text{ w.p. }\geq\frac{1}{8}$ &$1 \text{ w.p. }\frac{1}{2}$ &$1 \text{ w.p. }\frac{1}{2}$\\ \hline\end{tabular}
    \caption{Irreducibility} 
\end{table}
Thus, for every $i,j \in S,$ there exists some $n,m>0$ such that $p^n(i,j) >0$ and $p^m(j,i) >0,$ as you can see in the table above. 
    \end{solution}
    \item[(b)] Assume that the chain starts with \( X_0 = B \). What is the probability that \( X_3 = C \)?
\begin{solution}
    Using Wolfram, we see that 
    \[P^3 = 
\begin{pmatrix}
\frac{1}{4} & \frac{3}{16} & \frac{3}{8} & \frac{3}{16} \\
\frac{1}{4} & \frac{1}{4} & \frac{3}{8} & \frac{1}{8} \\
\frac{1}{2} & \frac{1}{8} & \frac{1}{4} & \frac{1}{8} \\
\frac{1}{4} & \frac{1}{8} & \frac{3}{8} & \frac{1}{4}
\end{pmatrix}
\] and thus 
\[\boxed{p^3(C, B)} = \frac{1}{4}\]

\end{solution}
    \item[(c)] Under the same assumption as in the previous part, what is the probability that \( X_3 = C \), \( X_4 = A \), and \( X_6 = B \) all occur?
\begin{solution}
    \[P^4 = 
\begin{pmatrix}
\frac{3}{8} & \frac{5}{32} & \frac{5}{16} & \frac{5}{32} \\
\frac{3}{8} & \frac{3}{16} & \frac{5}{16} & \frac{1}{8} \\
\frac{1}{4} & \frac{3}{16} & \frac{3}{8} & \frac{3}{16} \\
\frac{3}{8} & \frac{1}{8} & \frac{5}{16} & \frac{3}{16}
\end{pmatrix}
\]
and so 
\[\boxed{p^4(A, B) = \frac{5}{32}}.\]
\[P^6 = \begin{pmatrix}
\frac{11}{32} & \frac{21}{128} & \frac{21}{64} & \frac{21}{128} \\
\frac{11}{32} & \frac{11}{64} & \frac{21}{64} & \frac{5}{32} \\
\frac{5}{16} & \frac{11}{64} & \frac{11}{32} & \frac{11}{64} \\
\frac{11}{32} & \frac{5}{32} & \frac{21}{64} & \frac{11}{64}
\end{pmatrix}
\]
and so 
\[\boxed{p^6(B,B) = \frac{11}{64}}\]
\end{solution}
    \item[(d)] Compute the invariant probability vector for this Markov chain.
\begin{solution}
    We need to find some $\pi = (\pi_1, \pi_2, \pi_3, \pi_4)$ probability vector such that 
    \[\pi P  = \pi \iff P^T\pi^T = \pi^T \iff (P^T - I)\pi^T = 0\iff (4P^T - 4I)\pi^T = 0\] Thus, we need to find $N(4P^T - 4I):$
    \begin{align*}
        N(4P^T - 4I) &= 
        \left[\begin{array}{cccc|c}
            -4 & 1 & 2 & 1 & 0\\
            0 & -2 & 2 & 0 & 0 \\
            4 & 0 & -4 & 0 & 0\\
            0 & 0 & 2 & -2 & 0
        \end{array}\right]\\
        &\sim
        \left[\begin{array}{cccc|c}
            1 & -\frac{1}{4} & -\frac{1}{2} & -\frac{1}{4} & 0\\
            0 & -2 & 2 & 0 & 0 \\
            0 & 1 & -2 & 1 & 0\\
            0 & 0 & 2 & -2 & 0
        \end{array}\right]\\
        &\sim
        \left[\begin{array}{cccc|c}
            1 & -\frac{1}{4} & -\frac{1}{2} & -\frac{1}{4} & 0\\
            0 & 1 & -1 & 0 & 0 \\
            0 & 1 & -2 & 1 & 0\\
            0 & 0 & 2 & -2 & 0
        \end{array}\right]\\
        &\sim 
        \left[\begin{array}{cccc|c}
            1 & -\frac{1}{4} & -\frac{1}{2} & -\frac{1}{4} & 0\\
            0 & 1 & -1 & 0 & 0 \\
            0 & -1 & 2 & -1 & 0\\
            0 & 0 & 2 & -2 & 0
        \end{array}\right]\\
        &\sim
        \left[\begin{array}{cccc|c}
            1 & -\frac{1}{4} & -\frac{1}{2} & -\frac{1}{4} & 0\\
            0 & 1 & -1 & 0 & 0 \\
            0 & 0 & 1 & -1 & 0\\
            0 & 0 & 0 & 0 & 0
        \end{array}\right]\\
    \end{align*}
    Call $\pi^T = x$ for simplification. We get the following four equations:
    \begin{align}
        x_1 - \frac{1}{4}x_2 - \frac{1}{2}x_2 - \frac{1}{4}x_4 = 0
    \end{align}
    \begin{align}
        x_2 = x_3
    \end{align}
    \begin{align}
        x_3 = x_4
    \end{align}
    \begin{align}
        x_1 + x_2 + x_3 + x_4 = 1
    \end{align}
Plugging (2) and (3) into (1):
\begin{align}
    x_1 = x_2 = x_3 = x_4
\end{align}
By (4), we see that 
\[x = \begin{pmatrix}
    \frac{1}{4}\\
    \frac{1}{4}\\
    \frac{1}{4}\\
    \frac{1}{4}
\end{pmatrix}\] and thus $\pi = (\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4})$ is our invariant probability vector. 

\end{solution}
    \item[(e)] Let \( J_n \) denote the probability that at time \( n \) the system has been in state \( C \) exactly the same number of times it has been in state \( A \). Find
    \[
    \lim_{n \to \infty} J_n.
    \] Here again suppose that $X_0 = B$
\end{enumerate}
\begin{solution}
    Since $\{X_n\}$ is irreducible and aperiodic, we have that 
    \[\lim_{n\to \infty}p^n(C,B) = \pi_C, \qquad \lim_{n\to \infty}p^n(A,B) = \pi_A,\] where 
    \[\pi_C = \pi_A = \frac{1}{4}\] Thus, if 
    \[J_n = \bbP\{\sum_{k=1}^n \mathbbm{1}_{X_i = C}=  \sum_{k=1}^n \mathbbm{1}_{X_i = A}\}\]
\end{solution}
\end{problem}

\newpage
\section*{Problem 9}
\begin{problem}
    Let \( G \) be a finite, connected graph (i.e., any two vertices can be joined by a path of edges). Let \( X_n \) and \( Y_n \) be independent lazy random walks on \( G \), meaning that at each step \( X_n \) stays put with probability \( \frac{1}{2} \) and moves to a uniformly chosen neighboring vertex with probability \( \frac{1}{2} \), and similarly for \( Y_n \). In other words, the transition probabilities for \(\{X_n\}\) (and also for \(\{Y_n\}\)) are given by

\[
p(x, y) = \begin{cases} 
\frac{1}{2}, & \text{if } x = y \\
\frac{1}{2 \deg x}, & \text{if } x \neq y \text{ and } x \text{ and } y \text{ are joined by an edge} \\
0, & \text{otherwise}.
\end{cases}
\]



(a) Show that for any possible starting points \( X_0  = x_0\) and \( Y_0= y_0 \), with probability 1, there exists \( n \) such that \( X_n = Y_n \).
\begin{solution}
Denote \[p^n_X(i,j) =\bbP\{X_n = i \mid X_0 = j\}, \qquad P^n_Y(i,j)=\bbP\{Y_n = i \mid Y_0 = j\}.\] Fix some $j \in V(G).$ 
Enumerate the vertices of the graph by $V(G) = \{1,2,\dots, N\},$ and fix $j \in [N].$
    Since $G$ is connected, then it is clearly recurrent, and so $n_z = p_Y^{n_z}(z,y_0) >0$ for any $z\in V(G).$ Since $V(G)$ is finite, we know that $n := \max\{n_z \mid z\in V(G)\}$ exists. 
    
    We know that $p_X^n(j,j) > \frac{1}{2^n}.$ Let $\tau = \min\{k>0 \mid X_k = j\}.$ We know that $\bbP(\tau < \infty) = 1$ since $\{X_n\}$ is recurrent. By the strong Markov property, we lose no generality in assuming that $X_0 = j.$ Let $Y_0 = i.$ 

    
    
    Let $E_k := \{X_i = j\; \forall \; i \in \{nk-n, \dots, nk\} \}.$ Thus, $E_k$ is the event that $\{X_n\}$ stays at $j$ for $n$ turns, starting at turn $k.$ We know that, by the strong Markov property assumed above and the normal Markov property and time homogeneity, for any $s_0, \dots, s_{nk}\in V(G)$
    \[\bbP\{E_{k+1} \mid X_0 = s_0, \dots, X_{nk}= s_{n_k}\} = \bbP\{E_1 \mid X_0 = j\} > \frac{1}{2^n}.\] Thus, for any $M>0,$ we have that 
    \begin{align*}
        \bbP\{\text{not $E_k$ for any $k \in [M]$}\} &= \bbP\{\bigcap_{k=1}^M E_k^c\}\\
        &= \bbP\{E_M^c \mid \bigcap_{k=1}^{M-1} E_k^c \}\bbP\{\bigcap_{k=1}^{M-1} E_k^c\}\\
        &\leq (1-\frac{1}{2^n})\bbP\{\bigcap_{k=1}^{M-1} E_k^c\}\\
        &\leq (1-\frac{1}{2^n})^M\xrightarrow[M\to \infty]{} 0
    \end{align*}
    Thus, with probability $1,$ there exists some time interval of length $n$ such that $X = j$ for the entire interval.
    
    Let $F_k$ denote the probability that $Y$ hits $j$ at some point during the interval $nk - n.$ We know by a theorem in class that with probability $1,$ $F_k$ happens for some $M \in \bbN.$ 

    Since $Y$ and $X$ are independent, we get that 
    \[\bbP(E_M \cap F_M) = \bbP(E_M)\bbP(F_M) \xrightarrow[M\to \infty]{}1\]
\end{solution}

(b) Give an example of a graph \( G \) for which the conclusion of part (a) fails if \( X_n \) and \( Y_n \) are ordinary random walks rather than lazy random walks (i.e., \( X_{n+1} \) is never equal to \( X_n \) and \( Y_{n+1} \) is never equal to \( Y_n \).
\begin{solution}
Consider the following graph $G:$
    \begin{center}
\begin{tikzpicture}[scale=1.2, every node/.style={draw, circle, inner sep=1pt}]
    \node (A) at (-2,0) {1};
    \node (B) at (2,0) {2};

    \draw[-] (A) -- (B) node[midway, above left] {\textcolor{red}{${1}$}};
    \end{tikzpicture}
\end{center}
And consider when $X_0 = 1$ and $Y_0 = y.$ Then since $X_n = S\setminus\{Y_n\},$ the Markov chains will never be the same.
\end{solution}

\end{problem}

\end{document}