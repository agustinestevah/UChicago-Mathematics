\documentclass[11pt]{article}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{automata, positioning}
% NOTE: Add in the relevant information to the commands below; or, if you'll be using the same information frequently, add these commands at the top of paolo-pset.tex file. 
\newcommand{\name}{AgustÃ­n Esteva}
\newcommand{\email}{aesteva@uchicago.edu}
\newcommand{\classnum}{23500}
\newcommand{\subject}{Markov Chains, Martingales, and Brownian Motion}
\newcommand{\instructors}{Stephen Yearwood}
\newcommand{\assignment}{Problem Set 6}
\newcommand{\semester}{Spring 2025}
\newcommand{\duedate}{05/16/2025}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\Vol}{\text{Vol}}

%% blackboard bold math capitals
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}

%% script math capitals
\newcommand{\sA}{\mathscr{A}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sG}{\mathscr{G}}
\newcommand{\sH}{\mathscr{H}}
\newcommand{\sI}{\mathscr{I}}
\newcommand{\sJ}{\mathscr{J}}
\newcommand{\sK}{\mathscr{K}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sN}{\mathscr{N}}
\newcommand{\sO}{\mathscr{O}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\sS}{\mathscr{S}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sU}{\mathscr{U}}
\newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}}
\newcommand{\sX}{\mathscr{X}}
\newcommand{\sY}{\mathscr{Y}}
\newcommand{\sZ}{\mathscr{Z}}


\renewcommand{\emptyset}{\O}

\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\sm}{\setminus}


\newcommand{\sarr}{\rightarrow}
\newcommand{\arr}{\longrightarrow}

% NOTE: Defining collaborators is optional; to not list collaborators, comment out the line below.
%\newcommand{\collaborators}{Alyssa P. Hacker (\texttt{aphacker}), Ben Bitdiddle (\texttt{bitdiddle})}

\input{paolo-pset.tex}

% NOTE: To compile a version of this pset without problems, solutions, or reflections, uncomment the relevant line below.

%\excludeversion{problem}
%\excludeversion{solution}
%\excludeversion{reflection}

\begin{document}	
	
	% Use the \psetheader command at the beginning of a pset. 
	\psetheader
\section*{Problem 1}
Let \(\{X_n\}\) be a branching process started with a single individual, so that \(X_0 = 1\) and \(X_n\) is the number of individuals in generation \(n\). Let \(\{p_k\}_{k \geq 0}\) be the offspring distribution. Assume \(p_0 > 0\) and let \(\mu = \sum_{k=0}^\infty k p_k\) be the mean of the offspring distribution.

(a) Show that \(M_n = \mu^{-n} X_n\) is a martingale with respect to \(\mathcal{F}_n = \sigma(X_0, \ldots, X_n)\).
\begin{solution}
Note that $M_n$ is only well defined when $p_0 <1,$ so we will assume this. 

    $M_n$ is trivially $\mathcal{F}_n$ measurable. 

    Since $X_n \geq 0$ for any $n,$ we have that by a result in class about branching processes,
    \[\bbE[|M_n|] = \frac{1}{\mu^n}\bbE[|X_n|] =\frac{1}{\mu^n}\bbE[X_n]= \frac{1}{\mu^n}\mu^n \bbE[X_0] = 1  \]

    For the Martingale property, we let $\xi_i$ be the number of offspring produced by  individual $i$. We know that $\xi_i$ is independent of $X_n$ and we also infer that $X_{n} = \sum_{i=1}^{X_n}\xi_i = \sum_{i=1}^{X_n}\xi = X_n\xi.$ Thus, since $X_{n-1}$ is $\mathcal{F}_{n-1}$ measurable
    \begin{align*}
        \bbE[M_n \mid \mathcal{F}_{n-1}] &= \frac{1}{\mu^n}\bbE[X_n \mid \mathcal{F}_{n-1}]\\
        &= \frac{1}{\mu^n}\bbE[\sum_{i=1}^{X_n}\xi_i \mid \mathcal{F}_{n-1}]\\
        &=\frac{1}{\mu^n}\sum_{i=1}^{X_n}\bbE[\xi_i \mid \mathcal{F}_{n-1}]\\
        &=\frac{1}{\mu^n}\sum_{i=1}^{X_n}\bbE[\xi_i]\\
        &= \frac{1}{\mu^n}X_{n-1} \bbE[\xi]\\
        &= \frac{1}{\mu^n}X_{n-1} \mu\\
        &= M_{n-1}
    \end{align*}
\end{solution}

(b) Suppose that \(\mu = 1\). For each \(K \in \mathbb{N}\), use the optional stopping theorem applied to the stopping time
\[
T_K = \min\{n \geq 1 : X_n = 0 \text{ or } X_n \geq K\}
\]
to show that the probability that the population reaches at least \(K\) individuals before going extinct is at most \(1/K\).
\begin{solution}
    Assumption of the OST:
    \begin{itemize}
        \item Since $\mu = 1$ and $p_0 \neq 0,$ we have by a result in class that with probability $1,$ the population will go extinct. Thus, $X_n = 0$ for some large $n,$ and so  
        \[\bbP\{T_K < \infty\} = 1.\]
        \item Since $X_n$ is non-negative for all $n$ and $\mu = 1,$
        \[\bbE[|M_{T_K}|] = \bbE[M_{T_K}] =\mu^{-n} \bbE[X_{T_K}]  \leq \bbE\left[\sum_{i=1}^{R} \xi_i\right]=  \sum_{i=1}^R \bbE[\xi_i] = R\mu = R\]
        \item For $T_k \geq n,$ we have that $0<X_n < K$ and so
        \[\bbE[M_n \mathbbm{1}_{T_K \geq n}] \leq K \bbP\{T_K\ \geq n\} \to 0\] since the population must go extinct at some point.
    \end{itemize}
    By the OST, we have that 
    \[\bbE[M_{T_k}] = \bbE[M_0] = 1.\] But since $\mu = 1,$
    \[1 =\bbE[M_{T_K}] \geq 0P_L + KP_W \implies P_W \leq \frac{1}{K},\] where $P_W$ is the probability that we get to $K$ before $0$ and $P_L = 1-P_W$ is the probability we get to $0$ before we get to $K$.
\end{solution}

(c) Use part (b) to show that the extinction probability is 1 if \(\mu = 1\).
\begin{solution}
Since $P_L = 1-P_W,$ where 
\[P_L = \bbP\{X_n= 0 \text{ before }X_n = K\} = a = 1-P_W \geq 1-\frac{1}{K},\] then as $K\to \infty,$ 
\[1 \geq P_L \geq 1 \implies a = 1.\] Where the implication comes since $a = 1 \iff \exists n :X_n =0 \iff P_L = 1.$
\end{solution}

\newpage

\section*{Problem 2}
Let \(\{M_n\}\) be a martingale with respect to its natural filtration \(\{F_n\}\), and let \(\tau\) be a stopping time for \(M_n\) with \(\mathbb{E}[\tau] < \infty\). Suppose that there exists a constant \(K > 0\) such that \(|M_{n+1} - M_n| \leq K\) for all \(n\). Show that \(\mathbb{E}[M_\tau] = \mathbb{E}[M_0]\).
\begin{solution}
    It suffices to show we can apply the OST.
    \begin{itemize}
    \item Since $\bbE[\tau] < \infty$ and 
    \[\bbE[\tau] = \sum_{n=1}^\infty \bbP\{\tau \geq n\} < \infty.\] Since the series converges, then we necessarily have that $\sum_m^\infty \bbP\{\tau \geq n\} < \epsilon$ for large enough $m.$ Since the terms are all positive, we have that $\bbP\{\tau \geq n\} < \epsilon$ and thus $\bbP\{\tau \geq n\} \to 0,$ and so $\bbP\{\tau = \infty\} = 0$\footnote{For another proof of this, consider that by the Markov inequality, 
    \[\bbP\{\tau \geq n\} \leq \frac{\bbE[\tau]}{n} < \frac{C}{n} \to 0.\]
    }
    \item Using the triangle inequality, we have that 
    \begin{align*}
        \bbE[|M_\tau|] &= \bbE[|M_\tau - M_{\tau -1} + M_{\tau - 1} - M_{\tau - 2} + \dots M_0|]\\
        &\leq \bbE[|M_\tau -M_{\tau -1}| + |M_{\tau - 1} - M_{\tau - 2}| + \dots + |M_1 - M_0|]\\
        &\leq K\bbE[\tau] < \infty
    \end{align*} 
    \item By the first bullet point and similar logic to the second one to bound $M_n$ with $nK$, we have that 
    \[\bbE[M_n \mathbbm{1}_{\tau \geq n}] \leq nK\bbP\{\tau \geq n\}\to 0\]
    \end{itemize}
\end{solution}

\newpage

\section*{Problem 3 (Optional)}
Let \(N\) be a fixed positive integer, and let \(A\) be an arbitrary alphabet of size \(N\), which we view as a collection of characters \(\{s_i\}_{i \in \{1, \ldots, N\}}\). A string of length \(l \in \mathbb{N}\) is a concatenation of elements of \(A\), written as
\[
s_{j_1} s_{j_2} \ldots s_{j_l},
\]
where the indices \(j_k\) may or may not be distinct. Suppose each character \(s_i\) has probability \(p_i\) of being selected, and let \(S\) be an arbitrary (finite) string. We wish to compute the expected time until the string \(S\) is first observed, if we repeatedly sample according to the probabilities \(p_i\). To that end, let \(\{X_n\}_{n \in \mathbb{N}}\) denote the characters sampled up to time \(n\), and let

\[
T := \min\{n \geq 0 : X_{n-|S|+1} X_{n-|S|+2} \ldots X_n = S\}.
\]

Let \(L_n(S)\) be the first \(n\) (leftmost) characters of \(S\), and let \(R_n(S)\) be the last \(n\) (rightmost) characters of \(S\). Show that
\[
\mathbb{E}[T] = \sum_{i=1}^{|S|} \left(\prod_{j=1}^i p_j \right)^{-1} \mathbbm{1}_{\{R_i(S) = L_i(S)\}}.
\]

Give the analogous formula in the case of uniform sampling, and give a condition for a string \(S\) to maximize this expected time.

\newpage

\section*{Problem 4}
Suppose \(X_1, X_2, \ldots\) are independent random variables with distribution
\[
\mathbb{P}(X_j = 3) = 1 - \mathbb{P}\left(X_j = \frac{1}{3}\right) = \frac{1}{4}.
\]

Let \(M_0 = 1\) and for \(n \geq 1\),
\[
M_n = \prod_{j=1}^n X_j.
\]

(a) Show that \(M_n\) is a martingale with respect to \(F_n = \sigma(X_1, \ldots, X_n)\).
\begin{solution}
Note that since $X_1, \dots, X_n \sim \text{i.i.d}$, then 
\[\bbE[X_j] = 3\frac{1}{4} + \frac{1}{3}\frac{3}{4} = 1, \qquad \forall j  = 1,2,\dots\]
\begin{itemize}
    \item We see that $M_n$ is $\mathcal{F}_n-$measurable.
    \item Since the $X_i$s are independent, we can distribute the expectation over the product and see that  since everything is non-negative,
    \begin{align*}
        \bbE[|M_n|] &= \bbE[M_n]\\
        &= \bbE\left[\prod_{j=1}^n X_j\right]\\
        &= \prod_{j=1}^n\bbE[X_j]\\
        &= (\bbE[X])^n\\
        &= 1
    \end{align*} where we use the fact that $\bbE[X_j] = \bbE[X] = 1$ for any $j.$ 
    \item For the Martingale property, 
    \begin{align*}
        \bbE[M_n \mid \mathcal{F}_{n-1}]
        &= \bbE\left[\prod_{j=1}^{n-1}X_{j}\cdot  X_n\mid \mathcal{F}_{n-1}\right]\\
        &= \prod_{j=1}^{n-1}X_j \cdot \bbE[X_n \mid \mathcal{F}_{n-1]}]\\
        &= M_{n-1}\bbE[X_n]\\
        &= M_{n-1},
    \end{align*}
    where we use the fact that $X_i$ are $\mathcal{F}_{n-1}$ measurable for $i\leq n-1$ and that $X_n$ is independent of $X_i$ for $i< n.$
\end{itemize}



\end{solution}

(b) Use the optional stopping theorem to show that the probability that the value of \(M_n\) ever gets as high as $3^6$ equals \(3^{-6}\).
\begin{solution}
    Define $T_m = \min\{j : M_j = 3^6 \text{ or } M_j = 3^{-m}\}.$ We see that $T_m$ is a stopping time for $M_n.$ 
    \begin{itemize}
        \item We can reach $3^6$ in $6$ steps, and so 
        \[\bbP\{T_m  \geq 6\} \leq \frac{1}{4^6}\implies \bbP\{T_m \geq 6(2k)\}\leq \frac{1}{4^{6(2k)}} \implies \bbP\{T_m \geq k\} \leq \frac{1}{4^k},\] where we are bounding the probability by the event where $M_n$ keeps bouncing  between $1$ and $\frac{1}{3}$ $2k$ times until it decides to go $6$ times to $3^6$ and so as $k\to \infty,$ we find that 
        \[\bbP\{T_m \geq mk\} \to 0 = \bbP\{T_m = \infty\}.\]
        \item We see that by the previous bullet point
        \[\bbE[T_m] =  \sum_{k=1}^\infty \bbP\{T_m \geq k\} \leq \sum_{k=1}^\infty \frac{1}{4^k} < \infty\]
        \item We can bound
        \[\bbE[M_n \mathbbm{1}_{T_m \geq n}] \leq 3^6 \bbP\{T_m \geq n\} \to 0\]
    \end{itemize}
    Thus, we use the optional the optional stopping theorem that states that 
    \[\bbE[M_{T_m}] = \bbE[M_0] = 1.\] But
    \[1 = \bbE[M_{T_m}]= 3^6 P_{3^6} + 3^{-m}P_{3^{-m}},\] where 
    \[P_{3^6} = \bbP\{M_n = 3^6 \text{ before }M_n = 3^{-m}\}, \quad P_{3^m} = 1-P_{3^6}\] Taking $m\to \infty,$ we see that 
    \[P_{3^6} = \frac{1}{3^6}.\]
\end{solution}

(c) Show that there exists \(M_\infty\) such that, with probability one, \(M_n \to M_\infty\).
\begin{solution}
It suffices to show that $M_n$ satisfies the conditions of the MCT. We showed in the first step that $\bbE[|M_n|]  = 1,$ and so we are done since then the $|M_n| \leq 1$ uniformly. 
\end{solution}

(d) Does there exist a \(C < \infty\) such that for all \(n\), \(\mathbb{E}[M_n^2] \leq C\)?
\begin{solution}
    \textbf{No.} Consider first that 
    \[\bbE[X_j^2] = 9 \frac{1}{4} + \frac{1}{9}\frac{3}{4}  = \frac{7}{3}, \qquad \forall j = 1,2,\dots\]
    \begin{align*}
        \bbE[M_n^2] &= \bbE\left[\prod_{j=1}^n X_j^2\right]\\
        &= \prod_{j=1}^n\bbE[X]\\
        &= \bbE[X]^n\\
        &=\frac{7}{3}^n\\
        &\to \infty
    \end{align*}
\end{solution}

\newpage

\section*{Problem 5}
Define random variables \(\{X_n\}\) recursively by \(X_0 = 1\) and for \(n \geq 1\), \(X_n\) is sampled uniformly from \((0, X_{n-1})\).

(a) Show that \(M_n := 2^n X_n\) is a martingale.
\begin{solution}
    \begin{itemize}
        \item  It is left as an exercise to the grader that $M_n$ is $\mathcal{F}_n-$measurable
        \item We claim that 
        \[\bbE[X_n] = \frac{1}{2^n}.\] For $n=1,$ we have that $X_1 \sim U([0,X_0]) = U([0,1]),$ and so $\bbE[X_1] = \frac{1}{2}.$ Suppose this holds for a general $n=k,$ that $X_k \sim (0, X_{k-1})$ and $\bbE[X_k] = \frac{1}{2^k}.$ For $n = k+1,$ we see that 
        \[\bbE[X_{k+1}] =\frac{\bbE[X_k]}{2} = \frac{1}{2^{k+1}}\]
        Thus, 
        \[\bbE[|M_n|] = 2^n\bbE[X_n] = 1.\]
        \item For the martingale property, 
        \begin{align*}
            \bbE[M_n \mid \mathcal{F}_{n-1}] &= \bbE[2^n X_n \mid \mathcal{F}_{n-1}]\\
            &= 2^n \bbE[X_{n} \mid {X}_{n-1}]\\
            &= 2^n \frac{X_{n-1}}{n}\\
            &= 2^{n-1}X_{n-1}\\
            &= M_{n-1}
        \end{align*}
    \end{itemize}
\end{solution}

(b) Show that there exists \(M_\infty\) such that, with probability one, \(M_n \to M_\infty\).
\begin{solution}
    We showed above that $\bbE[M_n] = 1$ for all $n,$ and so by the MCT we are done.
\end{solution}

(c) Find \(M_\infty\). (Hint: Consider \(\log M_n\).)
\begin{solution}
We claim that $X_n \mid \mathcal{F}_{n-1} \sim U_n U_{n-1}\cdots U_1,$ where $U_1, \dots, U_n \sim \text{Uniform}(0,1).$ To see this, we can induct. For the $n+1$th case, we use the fact that $aU(0,1) = U(0,a)$
\[X_n \sim U(0, X_{n-1}) = X_{n-1}U(0,1) = U_{n+1} U_{n}\cdots U_1.\]
Thus.
\[M_n = 2^n X_n = 2^n \prod_{k=1}^n U.\] Hence,
\[\log M_n = n \log 2 + \sum_{k=1}^n \log U,\] where $\Tilde{U} \sim \text{Uniform} (0,2^n)$ We have that 
\[\frac{\log M_n}{n} = \log 2 + \frac{\sum_{k=1}^n \log U}{n} \to \log 2 + \bbE[\log U\] Where we can compute 
\[\bbE[\log U] = \int_{0}^1 \log x \,dx = -1\] and thus 
\[\frac{\log M_n}{n} \to -c\] for some $c\in \bbR$ and so $\log M_n \to -\infty,$ implying that $M_n \to 0 = M_\infty.$


\end{solution}

\newpage

\section*{Problem 6}
Suppose \(X\) is a standard normal random variable, i.e., \(X \sim \mathcal{N}(0,1)\).

(a) Let \(\Phi(x) := \mathbb{P}(X \leq x)\). Compute \(\mathbb{E}[X \Phi(X)]\).
\begin{solution}
    Brute forcing our way thru:
If we let 
\[u = \Phi(x) \implies du = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\]
\[dv = x \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \implies v = -\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\]
\begin{align*}
    \bbE[X \Phi(X)] &= \int_{-\infty}^\infty x f_X(x)\Phi(x)\,dx\\
    &= \int_{-\infty}^\infty x \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\Phi(x)\,dx\\
    &= [-\Phi(x)\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}]_{-\infty}^\infty + \int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \,dx\\
    &= \frac{1}{2\pi}\int_{-\infty}^\infty  e^{-x^2}\,dx
\end{align*}
If we let 
\[I = \int_{-\infty}^\infty e^{-x^2}\,dx,\] then if we let $r= x^2 + y^2$ and $\theta = \tan \frac{y}{x}$ we get the change of variables
\begin{align*}
I^2 &= \int_{-\infty}^\infty e^{-x^2}\,dx \int_{-\infty}^\infty e^{-y^2}\,dy\\
&= \int_{-\infty}^\infty\int_{-\infty}^\infty e^{-(x^2 + y^2)}\,dxdy\\
&= \int_0^{2\pi}\int_0^\infty e^{-r^2}r\,dr\,d\theta\\
&= \pi \int_0^\infty e^{-u}\,du\\
&= \pi \bigg[-e^{-u} + e^{-u}\bigg]_0^\infty\\
&= \pi,
\end{align*}
and so $\bbE[X \Phi(X)] = \frac{1}{2\sqrt{\pi}}$
\end{solution}


(b) Let \(Y = |X| + X\). Compute \(\mathbb{E}[Y^3]\).
\begin{solution}
    We use the moment generating function to note that 
    \[\bbE[Y^3] = M_Y^{(3)}(0),\] where 
    \[M_Y(t) := \bbE[e^{tY}] = \sum_{n=0}^\infty \frac{t^n \bbE[Y^n]}{n!}\] and so since $Y$ is continuous.
    \[M_Y(t) = \bbE[e^{tY}] = \int_{-\infty}^\infty e^{ty}f_Y(y)\,dy.\] To compute $f_Y(y),$ we first consider that (if $y \geq 0$) 
    \begin{align*}
        F_Y(y) &= \bbP\{Y \leq y\}\\
        &= \bbP\{|X| + X \leq y\}\\
        &= \bbP\{2X \leq y \mid X\geq 0\}\frac{1}{2} + \bbP\{0 \leq y\}\frac{1}{2}\\
        &= \frac{1}{2}\int_0^{\frac{y}{2}} f_X(x)\,dx + \frac{1}{2}
    \end{align*}
    and so \[f_Y(y)= \frac{1}{2\sqrt{2\pi}}e^{-\frac{y^2}{8}}\] Computing, 
    \begin{align*}
        \implies M_Y(t) &= \bbE[e^{tY}]\\ &= \int_{-\infty}^\infty e^{ty}\frac{1}{2\sqrt{2\pi}}e^{-\frac{y^2}{8}}\,dy\\
        &= \frac{1}{2\sqrt{2\pi}}\int_{-\infty}^\infty e^{ty - \frac{y^2}{8}}\,dy\\
        &= \frac{1}{2\sqrt{2\pi}}e^{\frac{t^2}{2}}\int_{-\infty}^\infty e^{-\frac{1}{8}(y- 4t)^2}\,du\\
        &= \frac{1}{2\sqrt{2\pi}}e^{\frac{t^2}{2}}\int_{-\infty}^\infty e^{-\left(\frac{u}{\sqrt{8}}\right)^2}\,du\\
        &= \frac{1}{\sqrt{\pi}}e^{\frac{t^2}{2}}\int_{-\infty}^\infty e^{-s^2}\,ds\\
        &= e^{\frac{t^2}{2}}
    \end{align*}
    Thus, taking the third derivative, 
    \[M_Y^{(3)}(0) = 0\]
\end{solution}

(c) Show that \(\text{Var}(\sin X) > \text{Var}(\cos X)\).
\begin{solution}
Since $\Var[X] = \bbE[X^2] - \bbE[X]^2,$ it suffices to find the first and second moments of $Y =\sin X$ and $Z = \cos X.$ We can compute
\begin{align*}
    \bbE[Z] &= \bbE[\cos X]\\
    &= \int_{-\infty}^\infty \cos x \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\,dx\\
    &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \cos x e^{-\frac{x^2}{2}}\,dx
\end{align*}
Letting 
\[G(t) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \cos (tx) e^{-\frac{x^2}{2}},\] we find that by  some integration by parts,
\begin{align*}
  G'(t) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty -x\sin(tx) e^{-\frac{x^2}{2}} \,dx\\
  &= -te^{-\frac{t^2}{2}}
\end{align*}
We are interested in $G(1).$ which Feynman tells us is given by 
\[G(1) = \int_{-\infty}^1 G'(t)\]
\end{solution}

\newpage

\section*{Problem 7}
Consider the infinite series

\[
\zeta(2k) = \sum_{n=1}^\infty \frac{1}{n^{2k}}, \qquad S(2k) = \sum_{n=0}^\infty \frac{1}{(2n + 1)^{2k}}.
\]

(a) Show that \(\zeta(2k) = \frac{2^{2k}}{2^{2k} - 1} S(2k)\).

\begin{solution}
Computing,
\begin{align*}
\zeta(2k) &= \sum_{n\text{ odd}} \frac{1}{n^{2k}} + \sum_{n \text{ even}} \frac{1}{n^{2k}}\\ &= \sum_{n=0} \frac{1}{(2n + 1)^{2k}} + \sum_{n=0}\frac{1}{(2n)^{2k}}  \\
&= S(2k) + \frac{1}{2^{2k}}\zeta(2k)
\end{align*}
and so rearranging
\[\zeta(2k) = \frac{1}{1-\frac{1}{2^{2k}}}S(2k) = \frac{2^{2k}}{2^{2k} - 1}S(2k)\]

\end{solution}


(b) Suppose \(X\) and \(Y\) are continuous, non-negative, independent random variables with densities \(f_X(x)\) and \(f_Y(y)\). Let \(Z = \frac{Y}{X}\). Show that the density of \(Z\) is given by
\[
f_Z(z) = \int_0^\infty x f_Y(zx) f_X(x) \, dx.
\]

\begin{solution}
If we let $f(x,y)$ be the joint density of $X$ and $Y,$ then 
\begin{align*}
    F_Z(z) &= \bbP\{Z \leq z\}\\
    &= \bbP\{\frac{Y}{X} \leq z\}\\
    &= \int_0^\infty \left(\int_0^{xz}f_{X}(x,y)\,dy\right)\,dx\\
    &= \int_0^\infty\left(\int_0^z xf(x, ux)\,du\right)\,dx\\
    &= \int_0^z \left(\int_0^\infty xf(x, ux)\,dx\right)\,du
\end{align*}
Differentiation, and using the fact that $f_{X,Y}(u,v) = f_X(u)f_Y(v)$ by independence,
\[f_Z(z) = \int_0^\infty x f(x,zx)\,dx = \int_0^\infty x f_X(x) f_Y(xz)\,dx\]
\end{solution}

(c) Assume that the random variables \(X\) and \(Y\) obey the Cauchy distribution, i.e.,

\[
f_X(x) = \frac{2}{\pi (1 + x^2)}, \qquad x \geq 0.
\]

Show that

\[
f_Z(z) = \frac{4 \log(z)}{\pi^2 (z^2 - 1)}.
\]

(d) By consider $\bbP\{Y \leq X\},$ Show that
\[
\int_0^1 \frac{\log z}{z^2 - 1} \, dz = \frac{\pi^2}{8}.
\]
\begin{solution}
    Since $Y$ and $X$ are i.i.d, then 
    \[\bbP\{Y < X\} = \frac{1}{2}.\] But we also have that
\[\bbP\{Y < X\} = \bbP\{Z < 1\} = F_Z(1) = \int_0^1 f_Z(z)\,dz = \frac{4}{\pi^2}\int_0^1 \frac{\log z}{z^2- 1}.\] Putting these equations together, we see that 
\[\frac{\log z}{z^2- 1}=  \frac{\pi^2}{8}\]
\end{solution}

(e) Use the previous part to deduce that \(S(2) = \frac{\pi^2}{8}\).
\begin{solution}
Since $z <1,$ we have a geometric series, we have that 
\[\sum_{n=0}^\infty z^n = \frac{1}{1-z} = \sum_{n \text{ odd}}z^n + \sum_{n \text{ even}}z^n\] But 
\[\sum_{n \text{ odd}}z^n= z + z^3 + z^5 +\cdots = z\sum_{n \text{ even}}z^n\] so then 
\[\frac{1}{1-z} =\sum_{n \text{ even}}z^n +  z\sum_{n \text{ even}}z^n = (1 + z)\sum_{n \text{ even}}z^n \] Rearranging:
\[\sum_{n \text{ even}}z^n = \frac{1}{z^2 -1} = -\frac{1}{1-z^2}.\] So then
\begin{align*}
    \int_0^1 \log z \frac{1}{z^2 -1} \,dz &= -\int_0^1 \log z\sum_{n \text{ even}}^\infty z^n\\
    &= -\sum_{n \text{ even}} \int_0^1\log (z) z^n\\
    &= -\sum_{n \text{ even}} -\frac{1}{(n+1)^2}\\
    &= \sum_{n \text{ even}} \frac{1}{(n+1)^2}\\
    &= \sum_{k=0}^\infty \frac{1}{(2k+1)^2}\\
    &= S(2)
\end{align*}

\end{solution}

(f) Conclude that
\[
\zeta(2) = \sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}.
\]
\begin{solution}
   Using part (a), we see that 
\begin{align*}
\sum_{n=1}^\infty \frac{1}{n^2} &= \zeta(2)\\
    &= \frac{2^2}{2^2-1}S(2)\\
    &= \frac{4}{3}\frac{\pi^2}{8}\\
    &= \frac{\pi^2}{6}
\end{align*}
\end{solution}




\end{document}